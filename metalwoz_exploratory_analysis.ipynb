{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "import copy\n",
    "import statistics\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "from transformers import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer, WEIGHTS_NAME, CONFIG_NAME\n",
    "from itertools import chain\n",
    "from ast import literal_eval\n",
    "from itertools import zip_longest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')\n",
    "# print(model) # check the architecture of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
    "                         'additional_special_tokens': ('<speaker1>', '<speaker2>')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens_(model, tokenizer):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    orig_num_tokens = len(tokenizer.encoder)\n",
    "    num_added_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN) # doesn't add if they are already there\n",
    "    if num_added_tokens > 0:\n",
    "        print(\"Tokens added to model: {}\".format(num_added_tokens))\n",
    "        model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use 5 special tokens:\n",
    "# - <bos> to indicate the start of the sequence\n",
    "# - <eos> to indicate the end of the sequence\n",
    "# - <speaker1> to indicate the beginning and the tokens of an utterance from the user\n",
    "# - <speaker2> to indicate the beginning and the tokens of an utterance from the bot\n",
    "# - <pad> as a padding token to build batches of sequences\n",
    "\n",
    "max_history = 2 # pairs of question/answer to be retained\n",
    "min_sentence_length = 1\n",
    "max_sentence_length = 20 # maximum length of a sentence produced by the model \n",
    "\n",
    "temperature = 0.75 # increases confidence in the most propable outputs \n",
    "\n",
    "use_cuda = False # whether to try to use cuda or not\n",
    "\n",
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
    "add_special_tokens_(model, tokenizer)       \n",
    "SPECIAL_TOKENS_IDS = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
    "TIME_FORMAT = '%Y%m%d_%H%M'\n",
    "\n",
    "\n",
    "# bos, eos, speaker1, speaker2 = \"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\"\n",
    "\n",
    "# MODEL_INPUTS = [\"input_ids\", \"mc_token_ids\", \"lm_labels\", \"mc_labels\", \"token_type_ids\"]\n",
    "# PADDED_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40478"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40478, 40479, 40481, 40482, 40480]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPECIAL_TOKENS_IDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Severe TODO \n",
    "H Parser είναι πιο χρονοβόρα από την extract pairs\n",
    "refactor το που γίνεται τι ώστε κάθε συνάρτηση να έχει ένα ρόλο \n",
    "το tokenization δεν κολλάει πολύ στην parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO download with torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = 'C:\\\\Users\\\\nikmand\\\\nikmand\\\\ncsr-chatbot\\\\'  # os.getcwd()\n",
    "\n",
    "# TODO new function for the tokenization process\n",
    "\n",
    "def parser(datafolder='metalwoz-v1\\\\dialoguesTest\\\\'): # rename to parser\n",
    "    \"\"\"\n",
    "    Function that reads files, keeps only 'turns' from each entry and tokenizes them\n",
    "\n",
    "    :param datafolder: path to the folder that contains the files\n",
    "    :return: a list that contains dialogs, each dialog is a list of lists where each of them represents the ids of a phrase \n",
    "    \"\"\"\n",
    "    dialogs = []\n",
    "    dialogs_len = []\n",
    "    files = list(glob.glob(workspace + datafolder + \"*.txt\"))\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            for line in f.readlines():\n",
    "                dialog = literal_eval(line)['turns'][1:] # keep only turns without the first sentence\n",
    "                dialog = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(phrase)) for phrase in dialog] # to lowercase is performed by the tokenizer \n",
    "                dialog_len = sum(len(phrase) for phrase in dialog)\n",
    "                dialogs.append(dialog) \n",
    "                dialogs_len.append(dialog_len)\n",
    "    return dialogs, dialogs_len        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogs, dialogs_len = parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3828\n",
      "269 17\n"
     ]
    }
   ],
   "source": [
    "print(len(dialogs))\n",
    "print(max(dialogs_len), min(dialogs_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_samples(samples, samples_len, percentile=90):\n",
    "    \n",
    "    samples_length = np.array(samples_len)\n",
    "    reasonable_length = np.percentile(samples_length, percentile)\n",
    "    print(reasonable_length)\n",
    "    \n",
    "    samples_red, samples_len_red = [], []\n",
    "    for sample, sample_len in zip(samples, samples_len):\n",
    "        if sample_len <= reasonable_length:\n",
    "            samples_red.append(sample)\n",
    "            samples_len_red.append(sample_len)\n",
    "    \n",
    "    return samples_red, samples_len_red  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141.0\n",
      "[61, 103, 73, 63, 100, 92, 131, 79, 99, 53, 96, 58, 46, 108, 63, 51, 106, 162, 36, 85, 74, 170, 90, 120, 37, 69, 25, 71, 127, 46]\n",
      "3828\n",
      "3446\n",
      "[61, 103, 73, 63, 100, 92, 131, 79, 99, 53, 96, 58, 46, 108, 63, 51, 106, 36, 85, 74, 90, 120, 37, 69, 25, 71, 127, 46, 51, 67]\n"
     ]
    }
   ],
   "source": [
    "dialogs_reduced, dialogs_len_reduced = filter_samples(dialogs, dialogs_len)\n",
    "print(dialogs_len[:30])\n",
    "print(len(dialogs)) \n",
    "print(len(dialogs_reduced)) \n",
    "print(dialogs_len_reduced[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pairs(dialogs = None, cache_file='cache_folder\\\\pairs.txt'):\n",
    "    \"\"\"\n",
    "    Function that creates pairs of input, output from dialogs, each dialogs corresponds now to many pairs.\n",
    "    \n",
    "    :param dialogs: a list with all the dialogs \n",
    "    :return a list whose elements are pairs of input, output  \n",
    "    \"\"\"\n",
    "    try: \n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            print(\"Cache file found loading content.\")\n",
    "            pairs = pickle.load(f)\n",
    "            return pairs\n",
    "    except: # cache file not created yet\n",
    "        print(\"Cache file not found. Start processing.\")\n",
    "        pairs = [] \n",
    "        for dialog in dialogs:\n",
    "            t_dict = {'input': []}\n",
    "            if len(dialog) % 2 != 0: # discard the last phrase if it was said by the user\n",
    "                dialog = dialog[:-1]\n",
    "            dialog_it = iter(dialog)\n",
    "            for i_phrase, o_phrase in zip_longest(dialog_it, dialog_it): # process phrases two by two\n",
    "                try:\n",
    "                    t_dict[\"input\"].append(t_dict[\"output\"])\n",
    "                except:\n",
    "                    pass\n",
    "                t_dict[\"input\"].append(i_phrase) # history\n",
    "                t_dict[\"output\"] = o_phrase\n",
    "                pairs.append(t_dict)\n",
    "                t_dict = copy.deepcopy(t_dict) # so future changes address only the new dict\n",
    "        with open(cache_file, \"wb\") as f:\n",
    "            pickle.dump(pairs, f)\n",
    "        return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache file found loading content.\n"
     ]
    }
   ],
   "source": [
    "pairs = extract_pairs(dialogs) #dialogs_reduced list of dictionaries of two keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_history(pairs, max_history=2): # seq len reduced from 263 to 181\n",
    "    pairs_len = []\n",
    "    for pair in pairs:\n",
    "        pair['input'] = pair['input'][-(2*max_history+1):] # at least one phrase is preserved\n",
    "        pair_len = sum(len(phrase) for phrase in pair['input']) + len(pair['output'])\n",
    "        pairs_len.append(pair_len)\n",
    "    return pairs, pairs_len   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs, pairs_len = adjust_history(pairs, max_history=2) # 7 is practically all history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.0\n"
     ]
    }
   ],
   "source": [
    "pairs_reduced, pairs_len_reduced = filter_samples(pairs, pairs_len) # reduces from 181 to 81 (history 2) or from 263 to 108\n",
    "# mean leangth with history 2 is  47 and max 181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pairs(pairs):\n",
    "   [] pair in pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19379\n",
      "17491\n",
      "{'input': [[488, 249, 1074, 12361, 15354, 504, 481, 3361], [249, 2518, 512, 1074, 246, 5358, 500, 481, 3361], [668, 5611, 239, 249, 1074, 688, 504, 2306], [525, 256, 252, 246, 1875, 4778], [912, 249, 1048, 246, 16219, 267]], 'output': [249, 2310, 256, 241, 2153, 485, 699, 512]}\n",
      "45\n",
      "181\n",
      "45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46.829093348469996"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(pairs))\n",
    "print(len(pairs_reduced))\n",
    "print(pairs[3])\n",
    "print(pairs_len[3])\n",
    "print(max(pairs_len))\n",
    "print(statistics.median(pairs_len))\n",
    "miiii = sum(pairs_len) / len(pairs_len)\n",
    "miiii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO να σώζεται σε αρχείο στην πιο κατάλληλη μορφή. Να δούμε αν βολεύει Pandas ή κάτι άλλο "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate a gpt pytorch model with pre-trained weights on language modelling task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "A helper class used to interact with the vocabulary in which our model has been pre-trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our language model have been pre-trained with a vocabulary of 40478 words.\n"
     ]
    }
   ],
   "source": [
    "print(\"Our language model have been pre-trained with a vocabulary of {} words.\".format(tokenizer.vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Εμείς έχουμε πάντα περιττού πλήθους history που αρχίζει και τελειώνει με speaker1 και reply που το λέει ο speaker2\n",
    "\n",
    "Για το input_ids: Η λογική είναι αναθέτει τον speaker2 κάθε φορά που μένει άρτιο πλήθος από διαλόγους(περιττό συνολικά μαζί με sos, βλέπε συνθήκη). Εμάς όλες μας οι λίστες έχουν άρτιο πλήθος οπότε θα ξεκινήσει με speaker2 ενώ θέλουμε speaker1. Τα επιμέρους αποτελεσματα όμως είναι συμβατά μεταξύ τους.\n",
    "\n",
    "Στο input_ids το i στο iter παίρνει τιμή i = seq_len - 2 (αφού ξεκινήσαμε από το δεύτερο στοιχείο το iteration)\n",
    "\n",
    "Για το token_type_ids: για κάθε μία λίστα κάνουμε iterate στα στοιχεία της, αν η θέση της λίστας είναι άρτια παίρνει speaker1 αλλιώς speaker2\n",
    "Στο token_type_ids: επειδή το πλήθος είναι περιττό με την προσθήκη του sos θα αλλάξει η σειρά και η πρώτη πρόταση θα πάει speaker2 και το reply speaker1\n",
    "\n",
    "Καταρχάς η αντιστοιχία που δίνουν οι ίδιοι στο δικό τους δεν ταιριάζει με αυτό που είχαμε σκεφτεί \n",
    "Κατά δεύτερο πρέπει να δούμε που θα μπει αν θα μπει το sos, αυτό μας δημιουργεί πρόβλημα αυτή τη στιγμή. Θα μπει μετά το tag του speaker ? \n",
    "\n",
    "είτε θα μπει μόνο του πριν τον speaker\n",
    "σε αυτή την περίπτωση θα πρέπει να παίρνει το tag του speaker1 στα tokens αυτό δε συμβαίνει τώρα και μας μπερδεύει τη σειρά \n",
    "\n",
    "για το label βάζει σε όλα τα inputs εκτός του reply -1, στο speaker2 του reply -1 και βάζει τα tokens του reply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τι ακριβώς θα δούμε με το validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_array = np.array(pairs_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13996 3500\n"
     ]
    }
   ],
   "source": [
    "pairs_train_l = pairs[:int(len(pairs)*0.8)]\n",
    "pairs_eval_l = pairs[int(len(pairs)*0.8):]\n",
    "print(len(pairs_train_l), len(pairs_eval_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Για διαχωρισμό σε train test\n",
    "υπάρχει κάτι που να δημιουργεί πρόβλημα;\n",
    "μπορεί να μας ενοχλεί ότι ζεύγη που έρχονται από διαφορετικούς διαλόγους θα χωριστούν; μας ενοχλεί αν δεν μπαίνουν με τη σειρά;\n",
    "αν δεν κάνουμε τυχαίο split κάποια domains δε θα εμφανίζονται στο train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_train, pairs_eval, pairs_train_len, pairs_eval_len = train_test_split(pairs_reduced, pairs_len_reduced, test_size=0.3, shuffle=True)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θέλω στο μοντέλο μου να δίνω τρία inputs όπως το παράγει η συνάρτηση build, συνεπώς αυτό θέλω να μου γυρίζει η συνάρτηση get item "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "TODO: pad on batch level \n",
    "\n",
    "in order to avoid padding to the global max_len we can define our own collate_fn\n",
    "which forms the samples into batches and call inside there the pad function.\n",
    "Samples should be allocated to batches based on their sequence length in order to\n",
    "minimize the need for padding.\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs(history, reply, tokenizer, with_eos=True):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    \n",
    "    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
    "    sequence = [[bos]] + history + [reply + ([eos] if with_eos else [])]\n",
    "    seq_len = len(sequence) # sequence λίστα από λίστες\n",
    "    sequence = [sequence[0]] + [[speaker2 if (seq_len-i) % 2 != 1 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "    \n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence)) # words\n",
    "    instance[\"token_type_ids\"] = [speaker1] + [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence[1:]) for _ in s] # for each word\n",
    "    instance[\"mask\"] = [1] * len(instance[\"input_ids\"]) \n",
    "    # TODO positional embeddings\n",
    "    instance[\"lm_labels\"] = ([-1] * sum(len(s) for s in sequence[:-1])) + [-1] + sequence[-1][1:]\n",
    "    \n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogDataset(Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dialog_pairs):\n",
    "        self.dataset = self.create_segments(dialog_pairs)\n",
    "        self.dataset = self.order_on_seq_length()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def create_segments(self, dialog_pairs):\n",
    "        dataset = []\n",
    "        for pair in dialog_pairs:\n",
    "            instance = create_model_inputs(pair['input'], pair['output'], tokenizer)\n",
    "            dataset.append(instance)\n",
    "        return dataset\n",
    "    \n",
    "    def order_on_seq_length(self):\n",
    "        return sorted(self.dataset, key=lambda x: len(x['input_ids']))\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return  self.dataset[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequenses(batch, pad_token=0):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    max_seq_len = max(len(entry[\"input_ids\"]) for entry in batch)\n",
    "    print(max_seq_len) # debug remove\n",
    "    for entry in batch:\n",
    "        for index_name in entry.keys():\n",
    "            if index_name == \"lm_labels\":\n",
    "                pad_token_ = -1\n",
    "            elif index_name == \"mask\":\n",
    "                pad_token_ = 0\n",
    "            else:\n",
    "                pad_token_ = pad_token\n",
    "            entry[index_name] =  entry[index_name] + [pad_token_] * (max_seq_len - len(entry[index_name]))\n",
    "  \n",
    "    return batch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    batch = pad_sequenses(batch, tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\n",
    "    \n",
    "    inputs = [torch.stack(list(map(lambda x: torch.from_numpy( \\\n",
    "        np.array(x[index_name])), batch)), dim=0) for index_name in batch[0].keys()]\n",
    "    \n",
    "    if use_cuda:\n",
    "        inputs = [input_tensor.cuda() for input_tensor in inputs]\n",
    "        \n",
    "    input_ids, mask, category_ids, label_ids = inputs    \n",
    "    \n",
    "    return input_ids.type(torch.LongTensor), mask, category_ids.type(torch.LongTensor), label_ids.type(torch.LongTensor)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = DialogDataset(pairs_train) \n",
    "validation_set = DialogDataset(pairs_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 64 \n",
    "EVAL_BATCH_SIZE = 100 \n",
    "\n",
    "dataloader_train = DataLoader(training_set, batch_size=TRAIN_BATCH_SIZE, shuffle=False, \n",
    "                              collate_fn=custom_collate_fn, num_workers=0) \n",
    "\n",
    "dataloader_valid = DataLoader(validation_set, batch_size=EVAL_BATCH_SIZE, shuffle=False,\n",
    "                              collate_fn=custom_collate_fn, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 10])\n",
      "tensor([[40478, 40481,  3569, 40482,  2229, 40479, 40480, 40480, 40480, 40480],\n",
      "        [40478, 40481,  3569, 40482,  3570, 40479, 40480, 40480, 40480, 40480],\n",
      "        [40478, 40481,  3570, 40482,  2229, 40479, 40480, 40480, 40480, 40480],\n",
      "        [40478, 40481,  3569, 40482,  3569, 40479, 40480, 40480, 40480, 40480],\n",
      "        [40478, 40481,  3569, 40482,  3570, 40479, 40480, 40480, 40480, 40480],\n",
      "        [40478, 40481,  2229, 40482,   685, 40479, 40480, 40480, 40480, 40480],\n",
      "        [40478, 40481,  3569, 40482,  3569, 40479, 40480, 40480, 40480, 40480],\n",
      "        [40478, 40481,  3569, 40482,  3570, 40479, 40480, 40480, 40480, 40480],\n",
      "        [40478, 40481,  3569, 40482,  3570, 40479, 40480, 40480, 40480, 40480],\n",
      "        [40478, 40481,  3569,   267, 40482,  3570, 40479, 40480, 40480, 40480],\n",
      "        [40478, 40481,  3569,   267, 40482,  3570, 40479, 40480, 40480, 40480],\n",
      "        [40478, 40481,  2229,   655, 40482,  3570, 40479, 40480, 40480, 40480],\n",
      "        [40478, 40481,  3569,   655, 40482,  3570, 40479, 40480, 40480, 40480],\n",
      "        [40478, 40481,  3569,   267, 40482,  3570, 40479, 40480, 40480, 40480],\n",
      "        [40478, 40481,  3569, 40482,  3570,   655, 40479, 40480, 40480, 40480],\n",
      "        [40478, 40481,  3569,   267, 40482,  3569, 40479, 40480, 40480, 40480],\n",
      "        [40478, 40481,  3569, 40482,  2229,   655, 40479, 40480, 40480, 40480],\n",
      "        [40478, 40481,  1150,   510, 40482,   881, 40479, 40480, 40480, 40480],\n",
      "        [40478, 40481,  4895, 20721, 40482,   773, 40479, 40480, 40480, 40480],\n",
      "        [40478, 40481,  3569,   267, 40482,  3570, 40479, 40480, 40480, 40480],\n",
      "        [40478, 40481,  3569, 40482,   685,   257, 40479, 40480, 40480, 40480],\n",
      "        [40478, 40481,  3570, 40482,  3569,   239, 40479, 40480, 40480, 40480],\n",
      "        [40478, 40481,  3569,   267, 40482,  3570, 40479, 40480, 40480, 40480],\n",
      "        [40478, 40481,  3570,   655, 40482,  3570, 40479, 40480, 40480, 40480],\n",
      "        [40478, 40481,  1233, 17177, 40482,   599,   720, 40479, 40480, 40480],\n",
      "        [40478, 40481,  3569,   267, 40482,  3570,   239, 40479, 40480, 40480],\n",
      "        [40478, 40481,   249,   966,  1150, 40482,   881, 40479, 40480, 40480],\n",
      "        [40478, 40481,  3569,  9330,   267, 40482,  3570, 40479, 40480, 40480],\n",
      "        [40478, 40481,  1796, 25303, 40482,   984, 25303, 40479, 40480, 40480],\n",
      "        [40478, 40481,  3569,   267, 40482,  3570,   267, 40479, 40480, 40480],\n",
      "        [40478, 40481,   685,   239, 40482,   685,  3491, 40479, 40480, 40480],\n",
      "        [40478, 40481,   685,  1572, 40482,   249,  3798, 40479, 40480, 40480],\n",
      "        [40478, 40481, 26558, 17177, 40482,   984,  4895, 40479, 40480, 40480],\n",
      "        [40478, 40481,  2446,   640,  2056, 40482,   685, 40479, 40480, 40480],\n",
      "        [40478, 40481,  2229, 40482,   718,   759,   249,  1150, 40479, 40480],\n",
      "        [40478, 40481,  1189, 17177, 40482, 17177,   640,  1189, 40479, 40480],\n",
      "        [40478, 40481,   249,  1048,  6702, 40482,   498,  1385, 40479, 40480],\n",
      "        [40478, 40481,  1048,   249,  4565,   257, 40482,   685, 40479, 40480],\n",
      "        [40478, 40481,  1150,   510,   556, 17177, 40482,  1304, 40479, 40480],\n",
      "        [40478, 40481,   249,   966,  1150, 40482,   556,   257, 40479, 40480],\n",
      "        [40478, 40481,  2401,  1424, 40482,   685,   512,   640, 40479, 40480],\n",
      "        [40478, 40481,  3570, 40482,   718,  1598,   249,  1150, 40479, 40480],\n",
      "        [40478, 40481,  1085,  2369,   609,  4895, 40482,  1132, 40479, 40480],\n",
      "        [40478, 40481,  3569,   267, 40482,  3569,   655,   267, 40479, 40480],\n",
      "        [40478, 40481,  2229,  1839,   599, 40482,   599,   257, 40479, 40480],\n",
      "        [40478, 40481,   481,  2104,   544,  1301, 40482,  1439, 40479, 40480],\n",
      "        [40478, 40481,   249,   604,   246,  1756, 40482,   773, 40479, 40480],\n",
      "        [40478, 40481,   544,   481,  1988,  1674, 40482,   685, 40479, 40480],\n",
      "        [40478, 40481,  4895, 20721, 40482, 15546,   507,   718,   257, 40479],\n",
      "        [40478, 40481,   249,   966,  1150,   556,   846, 40482,   881, 40479],\n",
      "        [40478, 40481,   670,   599, 40482,   670,   599,   512,   603, 40479],\n",
      "        [40478, 40481,   249,  1048,   963,  4565, 40482,   498,  1385, 40479],\n",
      "        [40478, 40481,  1233,   510,   531,  4895,  1572, 40482,   669, 40479],\n",
      "        [40478, 40481,  4303,   544,  1274,   651,   267, 40482,   685, 40479],\n",
      "        [40478, 40481, 17177,  1150,  1572,   239, 40482,   599, 17177, 40479],\n",
      "        [40478, 40481,   966,   485,  1233,   531,  4895, 40482,   881, 40479],\n",
      "        [40478, 40481, 34427, 17177, 40482,  4895,   485, 34427,   257, 40479],\n",
      "        [40478, 40481, 13988, 40482,  1304,   239,   599,  1807,   257, 40479],\n",
      "        [40478, 40481,   759,   512,  1150,  1175,   257, 40482,   685, 40479],\n",
      "        [40478, 40481,  3569, 40482,   718,   759,   249,  1150,   512, 40479],\n",
      "        [40478, 40481,   685, 40482,   599,   544,   704,  3775,   257, 40479],\n",
      "        [40478, 40481,   249,  1048,  1424,   239, 40482,   498,  1385, 40479],\n",
      "        [40478, 40481,   249,   966,   485, 10553,  1831, 40482,  1304, 40479],\n",
      "        [40478, 40481,  3569, 40482,   718,   759,   249,  1150,   257, 40479]])\n",
      "torch.Size([64, 10])\n",
      "tensor([[40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40482, 40482, 40482, 40482, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40482, 40482, 40482, 40482, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40482, 40482, 40482, 40482, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40482, 40482, 40482, 40482, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40482, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40482, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40482, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40482, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40482, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40482, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40482, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40481, 40482, 40482, 40482, 40480, 40480],\n",
      "        [40481, 40481, 40481, 40482, 40482, 40482, 40482, 40482, 40482, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40482, 40482, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40481, 40482, 40482, 40482, 40482, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40481, 40481, 40482, 40482, 40482, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40481, 40481, 40482, 40482, 40482, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40481, 40482, 40482, 40482, 40482, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40482, 40482, 40480],\n",
      "        [40481, 40481, 40481, 40482, 40482, 40482, 40482, 40482, 40482, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40481, 40481, 40482, 40482, 40482, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40482, 40482, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40481, 40482, 40482, 40482, 40482, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40481, 40481, 40482, 40482, 40482, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40481, 40481, 40482, 40482, 40482, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40481, 40481, 40482, 40482, 40482, 40480],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40482, 40482, 40482],\n",
      "        [40481, 40481, 40481, 40481, 40481, 40481, 40481, 40482, 40482, 40482],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40482, 40482, 40482],\n",
      "        [40481, 40481, 40481, 40481, 40481, 40481, 40482, 40482, 40482, 40482],\n",
      "        [40481, 40481, 40481, 40481, 40481, 40481, 40481, 40482, 40482, 40482],\n",
      "        [40481, 40481, 40481, 40481, 40481, 40481, 40481, 40482, 40482, 40482],\n",
      "        [40481, 40481, 40481, 40481, 40481, 40481, 40482, 40482, 40482, 40482],\n",
      "        [40481, 40481, 40481, 40481, 40481, 40481, 40481, 40482, 40482, 40482],\n",
      "        [40481, 40481, 40481, 40481, 40482, 40482, 40482, 40482, 40482, 40482],\n",
      "        [40481, 40481, 40481, 40482, 40482, 40482, 40482, 40482, 40482, 40482],\n",
      "        [40481, 40481, 40481, 40481, 40481, 40481, 40481, 40482, 40482, 40482],\n",
      "        [40481, 40481, 40481, 40482, 40482, 40482, 40482, 40482, 40482, 40482],\n",
      "        [40481, 40481, 40481, 40482, 40482, 40482, 40482, 40482, 40482, 40482],\n",
      "        [40481, 40481, 40481, 40481, 40481, 40481, 40482, 40482, 40482, 40482],\n",
      "        [40481, 40481, 40481, 40481, 40481, 40481, 40481, 40482, 40482, 40482],\n",
      "        [40481, 40481, 40481, 40482, 40482, 40482, 40482, 40482, 40482, 40482]],\n",
      "       dtype=torch.int32)\n",
      "torch.Size([64, 10])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "torch.Size([64, 10])\n",
      "tensor([[   -1,    -1,    -1,    -1,  2229, 40479,    -1,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,  3570, 40479,    -1,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,  2229, 40479,    -1,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,  3569, 40479,    -1,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,  3570, 40479,    -1,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,   685, 40479,    -1,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,  3569, 40479,    -1,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,  3570, 40479,    -1,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,  3570, 40479,    -1,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,  3570, 40479,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,  3570, 40479,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,  3570, 40479,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,  3570, 40479,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,  3570, 40479,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,  3570,   655, 40479,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,  3569, 40479,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,  2229,   655, 40479,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,   881, 40479,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,   773, 40479,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,  3570, 40479,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,   685,   257, 40479,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,  3569,   239, 40479,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,  3570, 40479,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,  3570, 40479,    -1,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,   599,   720, 40479,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,  3570,   239, 40479,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,    -1,   881, 40479,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,    -1,  3570, 40479,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,   984, 25303, 40479,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,  3570,   267, 40479,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,   685,  3491, 40479,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,   249,  3798, 40479,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,   984,  4895, 40479,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,    -1,   685, 40479,    -1,    -1],\n",
      "        [   -1,    -1,    -1,    -1,   718,   759,   249,  1150, 40479,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1, 17177,   640,  1189, 40479,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,    -1,   498,  1385, 40479,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,    -1,    -1,   685, 40479,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,    -1,    -1,  1304, 40479,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,    -1,   556,   257, 40479,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,   685,   512,   640, 40479,    -1],\n",
      "        [   -1,    -1,    -1,    -1,   718,  1598,   249,  1150, 40479,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,    -1,    -1,  1132, 40479,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,  3569,   655,   267, 40479,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,    -1,   599,   257, 40479,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,    -1,    -1,  1439, 40479,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,    -1,    -1,   773, 40479,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1,    -1,    -1,   685, 40479,    -1],\n",
      "        [   -1,    -1,    -1,    -1,    -1, 15546,   507,   718,   257, 40479],\n",
      "        [   -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,   881, 40479],\n",
      "        [   -1,    -1,    -1,    -1,    -1,   670,   599,   512,   603, 40479],\n",
      "        [   -1,    -1,    -1,    -1,    -1,    -1,    -1,   498,  1385, 40479],\n",
      "        [   -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,   669, 40479],\n",
      "        [   -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,   685, 40479],\n",
      "        [   -1,    -1,    -1,    -1,    -1,    -1,    -1,   599, 17177, 40479],\n",
      "        [   -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,   881, 40479],\n",
      "        [   -1,    -1,    -1,    -1,    -1,  4895,   485, 34427,   257, 40479],\n",
      "        [   -1,    -1,    -1,    -1,  1304,   239,   599,  1807,   257, 40479],\n",
      "        [   -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,   685, 40479],\n",
      "        [   -1,    -1,    -1,    -1,   718,   759,   249,  1150,   512, 40479],\n",
      "        [   -1,    -1,    -1,    -1,   599,   544,   704,  3775,   257, 40479],\n",
      "        [   -1,    -1,    -1,    -1,    -1,    -1,    -1,   498,  1385, 40479],\n",
      "        [   -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,  1304, 40479],\n",
      "        [   -1,    -1,    -1,    -1,   718,   759,   249,  1150,   257, 40479]])\n"
     ]
    }
   ],
   "source": [
    "for i_batch, (input_ids, mask, category_ids, label_ids) in enumerate(dataloader_train):\n",
    "    i+=1\n",
    "    print(type(input_ids))\n",
    "    print(input_ids.shape)\n",
    "    print(input_ids)\n",
    "    print(mask.shape)\n",
    "    print(mask)\n",
    "    print(category_ids.shape)\n",
    "    print(category_ids)\n",
    "    print(label_ids.shape)\n",
    "    print(label_ids)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Loss on train set: \t\t epoch 0 : 18.7202\n",
      "13\n",
      "Loss on validation set: \t epoch 0 : 4.5075\n",
      "New min validation loss: \t epoch 0 : 4.5075\n",
      "New checkpoint created\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "min_loss, max_patience, cur_patience = np.inf, 5, 0\n",
    "save_file = \"chatbot_{}.pkl\".format(datetime.now().strftime(TIME_FORMAT))\n",
    "\n",
    "if use_cuda and torch.cuda.is_available :\n",
    "    model.cuda()\n",
    "\n",
    "# loss_function = nn. check it την διαλέγει μόνο του?\n",
    "# αν δοθεί το labels αρχικοποιεί και χρησιμοποιεί εσωτερικά το crossEntropyLoss\n",
    "# να πούμε αναλυτικά τι κάνει σε πρώτη φάση, αν δε το γράψουμε χεράτα.\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3) # , weight_decay=0.001 # TODO review those values\n",
    "for epoch in range(epochs):\n",
    "    epoch_train_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for i_batch, (input_ids, attention_mask, category_ids, label_ids) in enumerate(dataloader_train):\n",
    "#         print(type(input_ids))\n",
    "#         print(input_ids.shape)\n",
    "#         print(category_ids.shape)\n",
    "#         print(label_ids.shape)\n",
    "        \n",
    "        loss, logits = model(input_ids, attention_mask, category_ids, labels=label_ids)\n",
    "        # print(type(outputs))\n",
    "        #  = outputs[:2]\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item()\n",
    "        break  \n",
    "    print(\"Loss on train set: \\t\\t epoch {} : {:.4f}\".format(epoch, epoch_train_loss/(i_batch + 1)))\n",
    "    \n",
    "    epoch_eval_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        for i_batch, (input_ids, attention_mask, category_ids, label_ids) in enumerate(dataloader_valid):\n",
    "            \n",
    "            loss, logits = model(input_ids, attention_mask, category_ids, labels=label_ids)\n",
    "            epoch_eval_loss += loss.item()\n",
    "            break\n",
    "            \n",
    "        print(\"Loss on validation set: \\t epoch {} : {:.4f}\".format(epoch, epoch_eval_loss/(i_batch + 1)))    \n",
    " \n",
    "    if (epoch_eval_loss >= min_loss):     # early stopping\n",
    "        cur_patience += 1\n",
    "        if (cur_patience >= max_patience):\n",
    "            print(\"Execution terminated due to Early Stopping at epoch: {}\".format(epoch))\n",
    "            break\n",
    "    else:\n",
    "        print(\"New min validation loss: \\t epoch {} : {:.4f}\".format(epoch, epoch_eval_loss/(i_batch + 1)))\n",
    "        checkpoint_state(model, tokenizer) #torch.save(model.state_dict(), save_file) # checkpointing\n",
    "        print(\"New checkpoint created\")\n",
    "        min_loss, cur_patience = epoch_eval_loss, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint_state(model, tokenizer, output_dir=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = workspace + 'metalwoz-v1\\\\checkpoint_{}'.format(datetime.now().strftime(TIME_FORMAT))\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(output_dir)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "    output_config_file = os.path.join(output_dir, CONFIG_NAME)  \n",
    "    \n",
    "    torch.save(model.state_dict(), output_model_file)\n",
    "    model.config.to_json_file(output_config_file)\n",
    "    tokenizer.save_vocabulary(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(output_dir='openai-gpt'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(output_dir)\n",
    "    tokenizer = OpenAIGPTTokenizer.from_pretrained(output_dir)\n",
    "    model = OpenAIGPTLMHeadModel.from_pretrained(output_dir)  \n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction with the bot - Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(history, reply_so_far):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # print(\"History is: {}\".format(history))\n",
    "    # print(\"Reply so far is: {}\".format(reply_so_far))\n",
    "    history = [tokenizer.encode(phrase) for phrase in history]\n",
    "    \n",
    "    instance = create_model_inputs(history, reply_so_far, tokenizer, with_eos=False)\n",
    "    \n",
    "    input_ids = torch.tensor(instance[\"input_ids\"]).unsqueeze(0)\n",
    "    token_type_ids = torch.tensor(instance[\"token_type_ids\"]).unsqueeze(0)\n",
    "    \n",
    "    return input_ids, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding(probs, logits, method=\"top_p\"):\n",
    "    \"\"\"\n",
    "    Functions that selects the next token to be emmited. Three different approaches are implemented: \n",
    "    \n",
    "    Greedy: the most probable token is selected.\n",
    "    Top-k : \n",
    "    Top-p : \n",
    "    \n",
    "    :param logits: \n",
    "    :param method: the decoding method to be used, Values={'greedy', 'top_k', 'top_p'}\n",
    "    :return: the selected token\n",
    "    \"\"\"\n",
    "    top_k = 40 # sample from the 100 most probable tokens based on their probs\n",
    "    top_p = 0.9 # sample from the n most probable tokens that have a cumulative probability at least 0.9 \n",
    "    \n",
    "    if method == \"greedy\":\n",
    "        return torch.argmax(probs).item()\n",
    "    \n",
    "    elif method == \"top_k\":        \n",
    "        prob_k = probs.topk(top_k)[0][-1].item() # value of the 100th most probable\n",
    "#         print(probs.topk(top_k)[0])\n",
    "#         print((probs < prob_k).nonzero().shape)\n",
    "        probs[probs < prob_k] = 0   # cut off the tail  \n",
    "        \n",
    "    elif method == \"top_p\":\n",
    "        probs_sorted, probs_indexes = probs.sort(dim=-1, descending=True) # start the cumulation from the most probable token in descending order\n",
    "        cum_probs = probs_sorted.cumsum(dim=-1)\n",
    "        \n",
    "        indices = cum_probs > top_p \n",
    "#         print(indices)\n",
    "#         print(indices.nonzero().shape)\n",
    "#         print(indices.nonzero())\n",
    "#         print(probs_sorted[:10])\n",
    "        indices[1:] = indices[:-1].clone()\n",
    "        indices[0] = 0 # at least one token is preserved \n",
    "        \n",
    "        probs[probs_indexes[indices]] = 0\n",
    "    \n",
    "    word = torch.multinomial(probs, 1).item()\n",
    "    # TODO handle the case that special token was emitted in the first pick\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_answer(history, model, tokenizer, method=\"top_p\"):\n",
    "    \"\"\"\n",
    "    Function that generates word by word the bot answer, based on user input and previous history.\n",
    "    \n",
    "    :param history: a list of past sentences and last user's input, in plain text\n",
    "    :param model: the model to be used for inference\n",
    "    :return: a list with the words of the answer in plain text \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    reply_so_far = []\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i in range(max_sentence_length):\n",
    "            \n",
    "            input_ids, category_ids = format_input(history, reply_so_far)\n",
    "            # print(\"Inputs ids are {}\".format(input_ids)) seems good\n",
    "            # print(\"Category ids are {}\".format(category_ids)) seems good\n",
    "            outputs = model(input_ids=input_ids, token_type_ids=category_ids)\n",
    "            logits = outputs[0]\n",
    "            logits = logits[0, -1, :] / temperature # keep last \n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            word = decoding(probs, logits, method=method) \n",
    "            \n",
    "            if word in SPECIAL_TOKENS_IDS: # we stop inference if we find a special token without emitting this token\n",
    "                print(\"Bot terminate sentence!\")\n",
    "                break\n",
    "            reply_so_far.append(word)\n",
    "            \n",
    "        answer_text = tokenizer.decode(reply_so_far, skip_special_tokens=True)    \n",
    "        return answer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_with_bot(model, tokenizer, method='top_p'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    bot_prompt = \"bot:>>> \"\n",
    "    user_prompt = \"user:>>> \"\n",
    "\n",
    "    history = []\n",
    "    print(bot_prompt + \"Hello how may I help you?\")\n",
    "    user_input = input(user_prompt)\n",
    "    \n",
    "    while user_input != \"\\q\": # TODO check if we need to truncate user input to not exceed max_length\n",
    "        \n",
    "        history.append(user_input)\n",
    "        answer = infer_answer(history, model, tokenizer, method=method)\n",
    "        history.append(answer)\n",
    "        \n",
    "        history = history[-(2*max_history+1):]  # keep the same history as in the training \n",
    "        \n",
    "        print(bot_prompt + answer)\n",
    "        \n",
    "        user_input = input(user_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikmand\\nikmand\\ncsr-chatbot\\metalwoz-v1\\checkpoint_20191114_1250\n"
     ]
    }
   ],
   "source": [
    "model_loaded ,tokenizer_loaded  = load_checkpoint(workspace + \"metalwoz-v1\\\\checkpoint_20191114_1250\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens added to model: 5\n"
     ]
    }
   ],
   "source": [
    "add_special_tokens_(model_loaded, tokenizer_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40478, 40479, 40481, 40482, 40480]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_loaded.convert_tokens_to_ids(SPECIAL_TOKENS) # θέλει ξανά να μπου τα σπέσιαλ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bot:>>> Hello how may I help you?\n",
      "user:>>> i need toothpaste\n",
      "Bot terminate sentence!\n",
      "bot:>>> \n",
      "user:>>> hey how are you?\n",
      "Bot terminate sentence!\n",
      "bot:>>> yes\n",
      "user:>>> tell me something.\n",
      "Bot terminate sentence!\n",
      "bot:>>> yes\n",
      "user:>>> \\q\n"
     ]
    }
   ],
   "source": [
    "interact_with_bot(model_loaded, tokenizer_loaded, method='top_p')\n",
    "test = \"could you book a ticket for me?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUG AREA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12547,) (3137,)\n",
      "{'input': [[13659, 2679], [1304, 239, 718, 1272, 12286, 587, 512, 966, 507, 485, 580, 257], [277], [566, 2499, 13659, 239, 587, 512, 604, 246, 5052, 5855, 257], [13103, 3597, 246, 3592]], 'output': [1304, 886, 507, 239, 544, 655, 246, 7537, 1807, 512, 640, 1081, 491, 257]}\n",
      "wwwwwww\n",
      "[[249, 966, 485, 5838, 531, 4895]]\n"
     ]
    }
   ],
   "source": [
    "print(pairs_train.shape, pairs_eval.shape)\n",
    "print(pairs_train[0])\n",
    "print(\"wwwwwww\")\n",
    "print(pairs_eval[0]['input'])\n",
    "\n",
    "print(pairs_train.shape, pairs_eval.shape)\n",
    "print(pairs_train[0])\n",
    "print(\"wwwwwww\")\n",
    "print(pairs_eval[0]['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =zip(pairs_train_len, pairs_train)\n",
    "\n",
    "for y in x:\n",
    "    print(y[0])\n",
    "    \n",
    "Z = [sample for length, sample in sorted(zip(pairs_train_len, pairs_train), key = lambda x: x[0])]\n",
    "print(len(Z))    \n",
    "\n",
    "training_set = DialogDataset(Z) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([8.1849, 7.7367, 5.3123, 5.3115, 4.0969], grad_fn=<TopkBackward>),\n",
      "indices=tensor([509, 558, 535, 656, 980]))\n",
      "tensor([  8.1849,   7.7367,   5.3123,  ..., -50.3099, -51.1483, -51.4548],\n",
      "       grad_fn=<SortBackward>)\n",
      "tensor([  509,   558,   535,  ..., 33567, 32509, 16443])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([8.1849, 7.7367, 5.3123, 5.3115, 4.0969], grad_fn=<TopkBackward>),\n",
       "indices=tensor([509, 558, 535, 656, 980]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(logits_e)\n",
    "print(torch.topk(logits_e, 5))\n",
    "log_sort, log_index = logits_e.sort(dim=-1, descending=True)\n",
    "print(log_sort)\n",
    "print(log_index)\n",
    "logits_e.topk(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_prob = probs.cumsum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cum_prob > 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[1:] = c[:-1].clone()\n",
    "#c[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[558]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([559])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(c == True).nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.3508, grad_fn=<NllLossBackward>)\n",
      "torch.Size([32, 124, 40483])\n",
      "<class 'tuple'>\n",
      "1\n",
      "torch.Size([32, 124, 40483])\n",
      "torch.Size([40483])\n",
      "509\n",
      "509\n"
     ]
    }
   ],
   "source": [
    "loss, logits = outputs[:2]\n",
    "print(loss)\n",
    "print(logits.shape)\n",
    "\n",
    "model.eval()\n",
    "outputs_e = model(input_ids=input_ids.type(torch.LongTensor), token_type_ids=category_ids.type(torch.LongTensor))\n",
    "print(type(outputs_e))\n",
    "\n",
    "print(len(outputs_e))\n",
    "print(outputs_e[0].shape)\n",
    "logits_e = outputs_e[0]\n",
    "logits_e = logits_e[0, -1, :] / temperature\n",
    "print(logits_e.shape)\n",
    "probs = F.softmax(logits_e, dim=-1)\n",
    "\n",
    "word = torch.argmax(probs)\n",
    "word_3 = torch.argmax(logits_e)\n",
    "# word_2 = torch.topk(probs, 1)[1]\n",
    "print(word.item()) \n",
    "print(word_3.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_segments(dialog_pairs):\n",
    "    dataset = []\n",
    "    for pair in dialog_pairs:\n",
    "        instance = build_input_from_segments(pair['input'], pair['output'], tokenizer)\n",
    "        dataset.append(instance)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def pad_sequenses(dataset, padding=0):\n",
    "    # TODO create mask for each entry\n",
    "    max_seq_len = max(len(entry[\"input_ids\"]) for entry in dataset)\n",
    "    print(max_seq_len)\n",
    "    for entry in dataset:\n",
    "        for index_name in entry.keys():\n",
    "            entry[index_name] =  entry[index_name] + [padding if index_name != \"lm_labels\" else -1] * (max_seq_len - len(entry[index_name]))\n",
    "        # entry[index_name] =  [ for index_name in entry.keys()]\n",
    "    return dataset   \n",
    "\n",
    "test_1 = create_segments(pairs_train)\n",
    "test_2 = pad_sequenses(test_1)\n",
    "test_1[0]\n",
    "test_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(40483, 768)"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(tokenizer.encoder)\n",
    "# model.resize_token_embeddings(new_num_tokens=40478 + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = OpenAIGPTDoubleHeadsModel.from_pretrained('openai-gpt')\n",
    "# print(model)\n",
    "# model_raw = OpenAIGPTModel.from_pretrained('openai-gpt')\n",
    "# print(model_raw) # without the last linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['could</w>', 'you</w>', 'book</w>', 'me</w>', 'a</w>', 'ticket</w>', '?</w>']\n",
      "[635, 512, 1861, 510, 246, 8194, 257]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "it = iter(a)\n",
    "testt = [it]*3\n",
    "\n",
    "test_tok = tokenizer.tokenize('could you book me a ticket?')\n",
    "print(test_tok)\n",
    "print(tokenizer.convert_tokens_to_ids(test_tok)) # all tokens must be lowercase \n",
    "tokenizer.encode('i i i i i i i i i i i i i i i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_len # what does it means ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      "token_type_ids\n",
      "lm_labels\n"
     ]
    }
   ],
   "source": [
    "for index_name in instance.keys():\n",
    "    print(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug cell\n",
    "def build_inputs(persona, history, reply):\n",
    "    # Build our sequence by adding delimiters and concatenating\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + [eos]]\n",
    "    sequence = [sequence[0]] + [ [speaker2 if (len(sequence)-i) % 2 else speaker1] + s\n",
    "                                for i, s in enumerate(sequence[1:])]\n",
    "    # Build our word, segments and position inputs from the sequence\n",
    "    words = list(chain(*sequence))                          # word tokens\n",
    "    segments = [speaker2 if i % 2 else speaker1             # segment tokens\n",
    "                for i, s in enumerate(sequence) for _ in s]\n",
    "    position = list(range(len(words)))                      # position tokens\n",
    "    return words, segments, position, sequence\n",
    "\n",
    "persona = [[\"i\", \"like\", \"playing\", \"football\", \".\"],\n",
    "           [\"i\", \"am\", \"from\", \"NYC\", \".\"]]\n",
    "history = [[\"hello\", \"how\", \"are\", \"you\", \"?\"],\n",
    "           [\"i\", \"am\", \"fine\", \"thanks\", \".\"]]\n",
    "reply = [\"great\", \"to\", \"hear\"]\n",
    "\n",
    "sequence = [[\"<bos>\"] + list(chain(*persona))] + history +  [reply + [\"<eos>\"]]\n",
    "sequence = [sequence[0]] + [ [speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "print(sequence)\n",
    "print(list(chain(*sequence))   )\n",
    "[speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
    "\n",
    "def build_input_from_segments_or(persona, history, reply, tokenizer, lm_labels=False, with_eos=True):\n",
    "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply. \"\"\"\n",
    "    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + ([eos] if with_eos else [])]  # chain: expands the lists  \n",
    "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence)) # words\n",
    "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
    "    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
    "    instance[\"lm_labels\"] = [-1] * len(instance[\"input_ids\"])\n",
    "    if lm_labels:\n",
    "        instance[\"lm_labels\"] = ([-1] * sum(len(s) for s in sequence[:-1])) + [-1] + sequence[-1][1:]\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[[40478], [40481, 249, 1048, 6702], [40482, 498, 1385, 512, 640], [40481, 488, 249, 1074, 12361, 15354, 504, 481, 3361], [40482, 249, 2518, 512, 1074, 246, 5358, 500, 481, 3361, 40479]]\n"
     ]
    }
   ],
   "source": [
    "# debug cell\n",
    "history = [[249, 1048, 6702], [498, 1385, 512, 640], [488, 249, 1074, 12361, 15354, 504, 481, 3361]]\n",
    "reply =  [249, 2518, 512, 1074, 246, 5358, 500, 481, 3361]\n",
    "\n",
    "instance = build_input_from_segments(history, reply, tokenizer)\n",
    "instance_or = build_input_from_segments_or(persona, history, reply, tokenizer, lm_labels=True)\n",
    "\n",
    "print(tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1]))\n",
    "instance_or[\"input_ids\"]\n",
    "instance_or[\"token_type_ids\"]\n",
    "instance[\"input_ids\"] \n",
    "instance[\"token_type_ids\"] \n",
    "instance = build_input_from_segments(persona, history, reply, tokenizer)\n",
    "\n",
    "### ασυμβίβαστα μεταξύ τους στο input_ids βγαίνει ότι το reply το είπε ο speaker1, ενώ στο token_type_ids ότι το είπε ο speaker2 (μάλλον για τον κώδικα του medium μόνο)\n",
    "\n",
    "lm_targets = ([-1] * sum(len(s) for s in sequence[:-1])) \\\n",
    "             + [-1] + tokenizer.convert_tokens_to_ids(sequence[-1][1:])\n",
    "\n",
    "lm_targets # στα labels tou language model έχουν τιμές μόνο τα tokens του reply.\n",
    "lm_distractor = [-1] * len(instance[\"input_ids\"])\n",
    "lm_distractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_old(logits, method=\"greedy\"):\n",
    "    \"\"\"\n",
    "    Functions that selects the next token to be emmited. Three different approaches are implemented: \n",
    "    \n",
    "    Greedy: the most probable token is selected.\n",
    "    Top-k : \n",
    "    Top-p : \n",
    "    \n",
    "    :param logits: \n",
    "    :param method: the decoding method to be used, Values={'greedy', 'top_k', 'top_p'}\n",
    "    :return: the selected token\n",
    "    \"\"\"\n",
    "    top_k = 100 # sample from the 100 most probable tokens based on their probs\n",
    "    top_p = 0.9 # sample from the n most probable tokens that have a cumulative probability > 0.9 \n",
    "    \n",
    "    if method == \"top_k\":        \n",
    "        logit_k = logits.topk(top_k)[0][-1].item() # value of the 100th most probable\n",
    "        logits[logits < logit_k] = -float('Inf')   # cut off the tail  \n",
    "        \n",
    "    elif method == \"top_p\":\n",
    "        logits_sorted, logits_indexes = logits.sort(dim=-1, descending=True) # start the cumulation from the most probable token in descending order\n",
    "        probs = F.softmax(logits_sorted, dim=-1)\n",
    "        cum_probs = probs.cumsum(dim=-1)\n",
    "        \n",
    "        indices = cum_probs > top_p \n",
    "        indices[1:] = indices[:-1].clone()\n",
    "        indices[0] = 0 # at least one token is preserved \n",
    "        \n",
    "        logits[logits_indexes[indices]] = -float('Inf')\n",
    "    \n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    word = torch.multinomial(probs, 1).item()\n",
    "    # TODO handle the case that special token was emitted in the first peek\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 249,\n",
       " 2518,\n",
       " 512,\n",
       " 1074,\n",
       " 246,\n",
       " 5358,\n",
       " 500,\n",
       " 481,\n",
       " 3361,\n",
       " 40479]"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance['lm_labels']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
