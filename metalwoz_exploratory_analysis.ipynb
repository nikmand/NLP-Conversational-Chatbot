{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "import copy\n",
    "import statistics\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "from transformers import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer, WEIGHTS_NAME, CONFIG_NAME\n",
    "from itertools import chain\n",
    "from ast import literal_eval\n",
    "from itertools import zip_longest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(output_dir='openai-gpt'):\n",
    "    \"\"\"\n",
    "    Loads GPT Model and the corresponding tokenizer from local checkpoint.\n",
    "    If no path is specified the pretrained weights on language modelling task are downloaded.\n",
    "    \n",
    "    :param output_dir: path to checkpoint \n",
    "    :return : model and tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = OpenAIGPTTokenizer.from_pretrained(output_dir)\n",
    "    model = OpenAIGPTLMHeadModel.from_pretrained(output_dir)  \n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_checkpoint() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO check number of parameters\n",
    "# print(model) # check the architecture of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
    "                         'additional_special_tokens': ('<speaker1>', '<speaker2>')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens_(model, tokenizer):\n",
    "    \"\"\"\n",
    "    Adds special tokens to the tokenizer and if they weren't present to the model also.\n",
    "    \"\"\"\n",
    "    orig_num_tokens = len(tokenizer.encoder)\n",
    "    num_added_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN) # doesn't add if they are already there\n",
    "    \n",
    "    if num_added_tokens > 0:\n",
    "        print(\"New tokens added to model: {}\".format(num_added_tokens))\n",
    "        model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens) \n",
    "    else:\n",
    "        print(\"No new tokens found! Nothing added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# various constants needed\n",
    "\n",
    "max_history = 2 # pairs of question/answer to be retained\n",
    "min_sentence_length = 1\n",
    "max_sentence_length = 20 # maximum length of a sentence produced by the model \n",
    "\n",
    "temperature = 0.75 # increases confidence in the most propable outputs \n",
    "\n",
    "use_cuda = False # whether to try to use cuda or not\n",
    "\n",
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
    "add_special_tokens_(model, tokenizer)  \n",
    "\n",
    "SPECIAL_TOKENS_IDS = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
    "TIME_FORMAT = '%Y%m%d_%H%M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40478, 40479, 40481, 40482, 40480]"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPECIAL_TOKENS_IDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Severe TODO \n",
    "H Parser είναι πιο χρονοβόρα από την extract pairs\n",
    "refactor το που γίνεται τι ώστε κάθε συνάρτηση να έχει ένα ρόλο \n",
    "το tokenization δεν κολλάει πολύ στην parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = 'C:\\\\Users\\\\nikmand\\\\nikmand\\\\ncsr-chatbot\\\\'  # os.getcwd()\n",
    "\n",
    "# TODO new function for the tokenization process\n",
    "\n",
    "def parser(datafolder='metalwoz-v1/dialoguesTest/', cache_file='cache_folder/pairs.txt'): \n",
    "    \"\"\"\n",
    "    Function that reads files, keeps only 'turns' from each entry and tokenizes them\n",
    "\n",
    "    :param datafolder: path to the folder that contains the files\n",
    "    :return: a list that contains dialogs, each dialog is a list of lists \n",
    "             where each of them represents the ids of a phrase,\n",
    "             a second list with number of words in each dialog \n",
    "    \"\"\"\n",
    "    try: \n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            print(\"Cache file found loading content.\")\n",
    "            pairs = pickle.load(f)\n",
    "            return pairs\n",
    "    except: # cache file not created yet\n",
    "    \n",
    "    dialogs, dialogs_len = [], []\n",
    "    files = list(glob.glob(os.path.join(workspace, datafolder ,\"*.txt\")))\n",
    "    \n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            for line in f.readlines():\n",
    "                \n",
    "                dialog = literal_eval(line)['turns'][1:] # keep only turns without the first sentence\n",
    "                dialog = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(phrase)) for phrase in dialog]\n",
    "                dialog_len = sum(len(phrase) for phrase in dialog)\n",
    "                \n",
    "                dialogs.append(dialog) \n",
    "                dialogs_len.append(dialog_len)\n",
    "                \n",
    "    return dialogs, dialogs_len        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pairs(dialogs = None, cache_file='cache_folder/pairs.txt'):\n",
    "    \"\"\"\n",
    "    Function that creates pairs of input, output from dialogs, each dialogs corresponds now to many pairs.\n",
    "    \n",
    "    :param dialogs: a list with all the dialogs \n",
    "    :return a list whose elements are pairs of input, output  \n",
    "    \"\"\"\n",
    "    try: \n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            print(\"Cache file found loading content.\")\n",
    "            pairs = pickle.load(f)\n",
    "            return pairs\n",
    "    except: # cache file not created yet\n",
    "        print(\"Cache file not found. Start processing.\")\n",
    "        pairs = [] \n",
    "        for dialog in dialogs:\n",
    "            t_dict = {'input': []}\n",
    "            if len(dialog) % 2 != 0: # discard the last phrase if it was said by the user\n",
    "                dialog = dialog[:-1]\n",
    "            dialog_it = iter(dialog)\n",
    "            for i_phrase, o_phrase in zip_longest(dialog_it, dialog_it): # process phrases two by two\n",
    "                try:\n",
    "                    t_dict[\"input\"].append(t_dict[\"output\"])\n",
    "                except:\n",
    "                    pass\n",
    "                t_dict[\"input\"].append(i_phrase) # history\n",
    "                t_dict[\"output\"] = o_phrase\n",
    "                pairs.append(t_dict)\n",
    "                t_dict = copy.deepcopy(t_dict) # so future changes address only the new dict\n",
    "        with open(cache_file, \"wb\") as f:\n",
    "            pickle.dump(pairs, f)\n",
    "        return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_history(pairs, max_history=2): # seq len reduced from 263 to 181\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    pairs_len = []\n",
    "    for pair in pairs:\n",
    "        pair['input'] = pair['input'][-(2*max_history+1):] # at least one phrase is preserved\n",
    "        pair_len = sum(len(phrase) for phrase in pair['input']) + len(pair['output'])\n",
    "        pairs_len.append(pair_len)\n",
    "    return pairs, pairs_len   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_samples(samples, samples_len, percentile=90):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    samples_length = np.array(samples_len)\n",
    "    reasonable_length = np.percentile(samples_length, percentile)\n",
    "    print(reasonable_length)\n",
    "    \n",
    "    samples_red, samples_len_red = [], []\n",
    "    for sample, sample_len in zip(samples, samples_len):\n",
    "        if sample_len <= reasonable_length:\n",
    "            samples_red.append(sample)\n",
    "            samples_len_red.append(sample_len)\n",
    "    \n",
    "    return samples_red, samples_len_red  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogs, dialogs_len = parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dialogs in the whole dataset: 3828\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of dialogs in the whole dataset: {}\".format(len(dialogs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache file found loading content.\n"
     ]
    }
   ],
   "source": [
    "pairs = extract_pairs(dialogs) # list of dictionaries of input history and bot's reply\n",
    "\n",
    "pairs, pairs_len = adjust_history(pairs, max_history=2) # keep only portion of the chat history to reduce seq_length\n",
    "\n",
    "pairs_reduced, pairs_len_reduced = filter_samples(pairs, pairs_len) # reduces from 181 to 81 (history 2) or from 263 to 108\n",
    "# mean leangth with history 2 is  47 and max 181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19379\n",
      "17491\n",
      "{'input': [[488, 249, 1074, 12361, 15354, 504, 481, 3361], [249, 2518, 512, 1074, 246, 5358, 500, 481, 3361], [668, 5611, 239, 249, 1074, 688, 504, 2306], [525, 256, 252, 246, 1875, 4778], [912, 249, 1048, 246, 16219, 267]], 'output': [249, 2310, 256, 241, 2153, 485, 699, 512]}\n",
      "45\n",
      "181\n",
      "45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46.829093348469996"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(pairs))\n",
    "print(len(pairs_reduced))\n",
    "print(pairs[3])\n",
    "print(pairs_len[3])\n",
    "print(max(pairs_len))\n",
    "print(statistics.median(pairs_len))\n",
    "miiii = sum(pairs_len) / len(pairs_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO να σώζεται σε αρχείο στην πιο κατάλληλη μορφή. Να δούμε αν βολεύει Pandas ή κάτι άλλο "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate a gpt pytorch model with pre-trained weights on language modelling task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "A helper class used to interact with the vocabulary in which our model has been pre-trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our language model have been pre-trained with a vocabulary of 40478 words.\n"
     ]
    }
   ],
   "source": [
    "print(\"Our language model have been pre-trained with a vocabulary of {} words.\".format(tokenizer.vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Εμείς έχουμε πάντα περιττού πλήθους history που αρχίζει και τελειώνει με speaker1 και reply που το λέει ο speaker2\n",
    "\n",
    "Για το input_ids: Η λογική είναι αναθέτει τον speaker2 κάθε φορά που μένει άρτιο πλήθος από διαλόγους(περιττό συνολικά μαζί με sos, βλέπε συνθήκη). Εμάς όλες μας οι λίστες έχουν άρτιο πλήθος οπότε θα ξεκινήσει με speaker2 ενώ θέλουμε speaker1. Τα επιμέρους αποτελεσματα όμως είναι συμβατά μεταξύ τους.\n",
    "\n",
    "Στο input_ids το i στο iter παίρνει τιμή i = seq_len - 2 (αφού ξεκινήσαμε από το δεύτερο στοιχείο το iteration)\n",
    "\n",
    "Για το token_type_ids: για κάθε μία λίστα κάνουμε iterate στα στοιχεία της, αν η θέση της λίστας είναι άρτια παίρνει speaker1 αλλιώς speaker2\n",
    "Στο token_type_ids: επειδή το πλήθος είναι περιττό με την προσθήκη του sos θα αλλάξει η σειρά και η πρώτη πρόταση θα πάει speaker2 και το reply speaker1\n",
    "\n",
    "Καταρχάς η αντιστοιχία που δίνουν οι ίδιοι στο δικό τους δεν ταιριάζει με αυτό που είχαμε σκεφτεί \n",
    "Κατά δεύτερο πρέπει να δούμε που θα μπει αν θα μπει το sos, αυτό μας δημιουργεί πρόβλημα αυτή τη στιγμή. Θα μπει μετά το tag του speaker ? \n",
    "\n",
    "είτε θα μπει μόνο του πριν τον speaker\n",
    "σε αυτή την περίπτωση θα πρέπει να παίρνει το tag του speaker1 στα tokens αυτό δε συμβαίνει τώρα και μας μπερδεύει τη σειρά \n",
    "\n",
    "για το label βάζει σε όλα τα inputs εκτός του reply -1, στο speaker2 του reply -1 και βάζει τα tokens του reply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τι ακριβώς θα δούμε με το validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_array = np.array(pairs_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13996 3500\n"
     ]
    }
   ],
   "source": [
    "pairs_train_l = pairs[:int(len(pairs)*0.8)]\n",
    "pairs_eval_l = pairs[int(len(pairs)*0.8):]\n",
    "print(len(pairs_train_l), len(pairs_eval_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Για διαχωρισμό σε train test\n",
    "υπάρχει κάτι που να δημιουργεί πρόβλημα;\n",
    "μπορεί να μας ενοχλεί ότι ζεύγη που έρχονται από διαφορετικούς διαλόγους θα χωριστούν; μας ενοχλεί αν δεν μπαίνουν με τη σειρά;\n",
    "αν δεν κάνουμε τυχαίο split κάποια domains δε θα εμφανίζονται στο train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_train, pairs_eval, pairs_train_len, pairs_eval_len = train_test_split(pairs_reduced, pairs_len_reduced, test_size=0.3, shuffle=True)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θέλω στο μοντέλο μου να δίνω τρία inputs όπως το παράγει η συνάρτηση build, συνεπώς αυτό θέλω να μου γυρίζει η συνάρτηση get item "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "TODO: pad on batch level \n",
    "\n",
    "in order to avoid padding to the global max_len we can define our own collate_fn\n",
    "which forms the samples into batches and call inside there the pad function.\n",
    "Samples should be allocated to batches based on their sequence length in order to\n",
    "minimize the need for padding.\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs(history, reply, tokenizer, with_eos=True):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    \n",
    "    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
    "    sequence = [[bos]] + history + [reply + ([eos] if with_eos else [])]\n",
    "    seq_len = len(sequence) # sequence λίστα από λίστες\n",
    "    sequence = [sequence[0]] + [[speaker2 if (seq_len-i) % 2 != 1 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "    \n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence)) # words\n",
    "    instance[\"token_type_ids\"] = [speaker1] + [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence[1:]) for _ in s] # for each word\n",
    "    instance[\"mask\"] = [1] * len(instance[\"input_ids\"]) \n",
    "    # TODO positional embeddings\n",
    "    instance[\"lm_labels\"] = ([-1] * sum(len(s) for s in sequence[:-1])) + [-1] + sequence[-1][1:]\n",
    "    \n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogDataset(Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dialog_pairs):\n",
    "        self.dataset = self.create_segments(dialog_pairs)\n",
    "        self.dataset = self.order_on_seq_length()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def create_segments(self, dialog_pairs):\n",
    "        dataset = []\n",
    "        for pair in dialog_pairs:\n",
    "            instance = create_model_inputs(pair['input'], pair['output'], tokenizer)\n",
    "            dataset.append(instance)\n",
    "        return dataset\n",
    "    \n",
    "    def order_on_seq_length(self):\n",
    "        return sorted(self.dataset, key=lambda x: len(x['input_ids']))\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return  self.dataset[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequenses(batch, pad_token=0):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    max_seq_len = max(len(entry[\"input_ids\"]) for entry in batch)\n",
    "    print(max_seq_len) # debug remove\n",
    "    for entry in batch:\n",
    "        for index_name in entry.keys():\n",
    "            if index_name == \"lm_labels\":\n",
    "                pad_token_ = -1\n",
    "            elif index_name == \"mask\":\n",
    "                pad_token_ = 0\n",
    "            else:\n",
    "                pad_token_ = pad_token\n",
    "            entry[index_name] =  entry[index_name] + [pad_token_] * (max_seq_len - len(entry[index_name]))\n",
    "  \n",
    "    return batch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    batch = pad_sequenses(batch, tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\n",
    "    \n",
    "    inputs = [torch.stack(list(map(lambda x: torch.from_numpy( \\\n",
    "        np.array(x[index_name])), batch)), dim=0) for index_name in batch[0].keys()]\n",
    "\n",
    "    inputs = [input_tensor.type(torch.LongTensor) for input_tensor in inputs]\n",
    "\n",
    "    if use_cuda:\n",
    "        inputs = [input_tensor.cuda() for input_tensor in inputs]\n",
    "        \n",
    "    # input_ids, mask, category_ids, label_ids = inputs    \n",
    "\n",
    "    return inputs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = DialogDataset(pairs_train) \n",
    "validation_set = DialogDataset(pairs_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 64 \n",
    "EVAL_BATCH_SIZE = 100 \n",
    "\n",
    "dataloader_train = DataLoader(training_set, batch_size=TRAIN_BATCH_SIZE, shuffle=False, \n",
    "                              collate_fn=custom_collate_fn, num_workers=0) \n",
    "\n",
    "dataloader_valid = DataLoader(validation_set, batch_size=EVAL_BATCH_SIZE, shuffle=False,\n",
    "                              collate_fn=custom_collate_fn, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint_state(model, tokenizer, output_dir=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = workspace + 'metalwoz-v1\\\\checkpoint_{}'.format(datetime.now().strftime(TIME_FORMAT))\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(output_dir)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "    output_config_file = os.path.join(output_dir, CONFIG_NAME)  \n",
    "    \n",
    "    torch.save(model.state_dict(), output_model_file)\n",
    "    model.config.to_json_file(output_config_file)\n",
    "    tokenizer.save_vocabulary(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Loss on train set: \t\t epoch 0 : 18.7202\n",
      "13\n",
      "Loss on validation set: \t epoch 0 : 4.5075\n",
      "New min validation loss: \t epoch 0 : 4.5075\n",
      "New checkpoint created\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "min_loss, max_patience, cur_patience = np.inf, 5, 0\n",
    "\n",
    "if use_cuda and torch.cuda.is_available :\n",
    "    model.cuda()\n",
    "\n",
    "# loss_function = nn. check it την διαλέγει μόνο του?\n",
    "# αν δοθεί το labels αρχικοποιεί και χρησιμοποιεί εσωτερικά το crossEntropyLoss\n",
    "# να πούμε αναλυτικά τι κάνει σε πρώτη φάση, αν δε το γράψουμε χεράτα.\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3) # , weight_decay=0.001 # TODO review those values\n",
    "for epoch in range(epochs):\n",
    "    epoch_train_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for i_batch, (input_ids, attention_mask, category_ids, label_ids) in enumerate(dataloader_train):\n",
    "#         print(type(input_ids))\n",
    "#         print(input_ids.shape)\n",
    "#         print(category_ids.shape)\n",
    "#         print(label_ids.shape)\n",
    "        \n",
    "        loss, logits = model(input_ids, attention_mask, category_ids, labels=label_ids)\n",
    "        # print(type(outputs))\n",
    "        #  = outputs[:2]\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item()\n",
    "        break  \n",
    "    print(\"Loss on train set: \\t\\t epoch {} : {:.4f}\".format(epoch, epoch_train_loss/(i_batch + 1)))\n",
    "    \n",
    "    epoch_eval_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        for i_batch, (input_ids, attention_mask, category_ids, label_ids) in enumerate(dataloader_valid):\n",
    "            \n",
    "            loss, logits = model(input_ids, attention_mask, category_ids, labels=label_ids)\n",
    "            epoch_eval_loss += loss.item()\n",
    "            break\n",
    "            \n",
    "        print(\"Loss on validation set: \\t epoch {} : {:.4f}\".format(epoch, epoch_eval_loss/(i_batch + 1)))    \n",
    " \n",
    "    if (epoch_eval_loss >= min_loss):     # early stopping\n",
    "        cur_patience += 1\n",
    "        if (cur_patience >= max_patience):\n",
    "            print(\"Execution terminated due to Early Stopping at epoch: {}\".format(epoch))\n",
    "            break\n",
    "    else:\n",
    "        print(\"New min validation loss: \\t epoch {} : {:.4f}\".format(epoch, epoch_eval_loss/(i_batch + 1)))\n",
    "        checkpoint_state(model, tokenizer) #torch.save(model.state_dict(), save_file) # checkpointing\n",
    "        print(\"New checkpoint created\")\n",
    "        min_loss, cur_patience = epoch_eval_loss, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction with the bot - Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(history, reply_so_far):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # print(\"History is: {}\".format(history))\n",
    "    # print(\"Reply so far is: {}\".format(reply_so_far))\n",
    "    history = [tokenizer.encode(phrase) for phrase in history]\n",
    "    \n",
    "    instance = create_model_inputs(history, reply_so_far, tokenizer, with_eos=False)\n",
    "    \n",
    "    input_ids = torch.tensor(instance[\"input_ids\"]).unsqueeze(0)\n",
    "    token_type_ids = torch.tensor(instance[\"token_type_ids\"]).unsqueeze(0)\n",
    "    \n",
    "    return input_ids, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding(probs, logits, method=\"top_p\"):\n",
    "    \"\"\"\n",
    "    Functions that selects the next token to be emmited. Three different approaches are implemented: \n",
    "    \n",
    "    Greedy: the most probable token is selected.\n",
    "    Top-k : \n",
    "    Top-p : \n",
    "    \n",
    "    :param logits: \n",
    "    :param method: the decoding method to be used, Values={'greedy', 'top_k', 'top_p'}\n",
    "    :return: the selected token\n",
    "    \"\"\"\n",
    "    top_k = 40 # sample from the 100 most probable tokens based on their probs\n",
    "    top_p = 0.9 # sample from the n most probable tokens that have a cumulative probability at least 0.9 \n",
    "    \n",
    "    if method == \"greedy\":\n",
    "        return torch.argmax(probs).item()\n",
    "    \n",
    "    elif method == \"top_k\":        \n",
    "        prob_k = probs.topk(top_k)[0][-1].item() # value of the 100th most probable\n",
    "#         print(probs.topk(top_k)[0])\n",
    "#         print((probs < prob_k).nonzero().shape)\n",
    "        probs[probs < prob_k] = 0   # cut off the tail  \n",
    "        \n",
    "    elif method == \"top_p\":\n",
    "        probs_sorted, probs_indexes = probs.sort(dim=-1, descending=True) # start the cumulation from the most probable token in descending order\n",
    "        cum_probs = probs_sorted.cumsum(dim=-1)\n",
    "        \n",
    "        indices = cum_probs > top_p \n",
    "#         print(indices)\n",
    "#         print(indices.nonzero().shape)\n",
    "#         print(indices.nonzero())\n",
    "#         print(probs_sorted[:10])\n",
    "        indices[1:] = indices[:-1].clone()\n",
    "        indices[0] = 0 # at least one token is preserved \n",
    "        \n",
    "        probs[probs_indexes[indices]] = 0\n",
    "    \n",
    "    word = torch.multinomial(probs, 1).item()\n",
    "    # TODO handle the case that special token was emitted in the first pick\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_answer(history, model, tokenizer, method=\"top_p\"):\n",
    "    \"\"\"\n",
    "    Function that generates word by word the bot answer, based on user input and previous history.\n",
    "    \n",
    "    :param history: a list of past sentences and last user's input, in plain text\n",
    "    :param model: the model to be used for inference\n",
    "    :return: a list with the words of the answer in plain text \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    reply_so_far = []\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i in range(max_sentence_length):\n",
    "            \n",
    "            input_ids, category_ids = format_input(history, reply_so_far)\n",
    "            # print(\"Inputs ids are {}\".format(input_ids)) seems good\n",
    "            # print(\"Category ids are {}\".format(category_ids)) seems good\n",
    "            outputs = model(input_ids=input_ids, token_type_ids=category_ids)\n",
    "            logits = outputs[0]\n",
    "            logits = logits[0, -1, :] / temperature # keep last \n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            word = decoding(probs, logits, method=method) \n",
    "            \n",
    "            if word in SPECIAL_TOKENS_IDS: # we stop inference if we find a special token without emitting this token\n",
    "                print(\"Bot terminate sentence!\")\n",
    "                break\n",
    "            reply_so_far.append(word)\n",
    "            \n",
    "        answer_text = tokenizer.decode(reply_so_far, skip_special_tokens=True)    \n",
    "        return answer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_with_bot(model, tokenizer, method='top_p'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    bot_prompt = \"bot:>>> \"\n",
    "    user_prompt = \"user:>>> \"\n",
    "\n",
    "    history = []\n",
    "    print(bot_prompt + \"Hello how may I help you?\")\n",
    "    user_input = input(user_prompt)\n",
    "    \n",
    "    while user_input != \"\\q\": # TODO check if we need to truncate user input to not exceed max_length\n",
    "        \n",
    "        history.append(user_input)\n",
    "        answer = infer_answer(history, model, tokenizer, method=method)\n",
    "        history.append(answer)\n",
    "        \n",
    "        history = history[-(2*max_history+1):]  # keep the same history as in the training \n",
    "        \n",
    "        print(bot_prompt + answer)\n",
    "        \n",
    "        user_input = input(user_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikmand\\nikmand\\ncsr-chatbot\\metalwoz-v1\\checkpoint_20191114_1250\n"
     ]
    }
   ],
   "source": [
    "model_loaded ,tokenizer_loaded  = load_checkpoint(workspace + \"metalwoz-v1\\\\checkpoint_20191114_1250\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens added to model: 5\n"
     ]
    }
   ],
   "source": [
    "add_special_tokens_(model_loaded, tokenizer_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40478, 40479, 40481, 40482, 40480]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_loaded.convert_tokens_to_ids(SPECIAL_TOKENS) # θέλει ξανά να μπου τα σπέσιαλ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bot:>>> Hello how may I help you?\n",
      "user:>>> i need toothpaste\n",
      "Bot terminate sentence!\n",
      "bot:>>> \n",
      "user:>>> hey how are you?\n",
      "Bot terminate sentence!\n",
      "bot:>>> yes\n",
      "user:>>> tell me something.\n",
      "Bot terminate sentence!\n",
      "bot:>>> yes\n",
      "user:>>> \\q\n"
     ]
    }
   ],
   "source": [
    "interact_with_bot(model_loaded, tokenizer_loaded, method='top_p')\n",
    "test = \"could you book a ticket for me?\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
