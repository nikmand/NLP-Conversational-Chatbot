{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "import copy\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import OpenAIGPTModel, OpenAIGPTLMHeadModel, OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer\n",
    "from itertools import chain\n",
    "from ast import literal_eval\n",
    "from itertools import zip_longest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')\n",
    "# print(model) # check the architecture of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
    "                         'additional_special_tokens': ('<speaker1>', '<speaker2>')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens_(model, tokenizer):\n",
    "    \"\"\" Add special tokens to the tokenizer and the model if they have not already been added. \"\"\"\n",
    "    orig_num_tokens = len(tokenizer.encoder)\n",
    "    num_added_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN) # doesn't add if they are already there\n",
    "    if num_added_tokens > 0:\n",
    "        print(\"Tokens added to model: {}\".format(num_added_tokens))\n",
    "        model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use 5 special tokens:\n",
    "# - <bos> to indicate the start of the sequence\n",
    "# - <eos> to indicate the end of the sequence\n",
    "# - <speaker1> to indicate the beginning and the tokens of an utterance from the user\n",
    "# - <speaker2> to indicate the beginning and the tokens of an utterance from the bot\n",
    "# - <pad> as a padding token to build batches of sequences\n",
    "\n",
    "max_history = 2 # pairs of question/answer to be retained\n",
    "max_sentence_length = 20 # maximum length of a sentence produced by the model \n",
    "\n",
    "temperature = 0.65 # increase confidence in the most propable outputs \n",
    "\n",
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
    "add_special_tokens_(model, tokenizer)       \n",
    "SPECIAL_TOKENS_IDS = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
    "\n",
    "\n",
    "# bos, eos, speaker1, speaker2 = \"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\"\n",
    "\n",
    "# MODEL_INPUTS = [\"input_ids\", \"mc_token_ids\", \"lm_labels\", \"mc_labels\", \"token_type_ids\"]\n",
    "# PADDED_INPUTS = [\"input_ids\", \"lm_labels\", \"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40478, 40479, 40481, 40482, 40480]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPECIAL_TOKENS_IDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Severe TODO \n",
    "H Parser είναι πιο χρονοβόρα από την extract pairs\n",
    "refactor το που γίνεται τι ώστε κάθε συνάρτηση να έχει ένα ρόλο \n",
    "το tokenization δεν κολλάει πολύ στην parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO download with torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = 'C:\\\\Users\\\\nikmand\\\\nikmand\\\\ncsr-chatbot\\\\'  # os.getcwd()\n",
    "\n",
    "# TODO new function for the tokenization process\n",
    "\n",
    "def parser(datafolder='metalwoz-v1\\\\dialoguesTest\\\\'): # rename to parser\n",
    "    \"\"\"\n",
    "    Function that reads files, keeps only 'turns' from each entry and tokenizes them\n",
    "\n",
    "    :param datafolder: path to the folder that contains the files\n",
    "    :return: a list that contains dialogs, each dialog is a list of lists where each of them represents the ids of a phrase \n",
    "    \"\"\"\n",
    "    dialogs = []\n",
    "    dialogs_len = []\n",
    "    files = list(glob.glob(workspace + datafolder + \"*.txt\"))\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            for line in f.readlines():\n",
    "                dialog = literal_eval(line)['turns'][1:] # keep only turns without the first sentence\n",
    "                dialog = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(phrase)) for phrase in dialog] # to lowercase is performed by the tokenizer \n",
    "                dialog_len = sum(len(phrase) for phrase in dialog)\n",
    "                dialogs.append(dialog) \n",
    "                dialogs_len.append(dialog_len)\n",
    "    return dialogs, dialogs_len        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-886bde5827ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdialogs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdialogs_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'parser' is not defined"
     ]
    }
   ],
   "source": [
    "dialogs, dialogs_len = parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3461\n",
      "269 17\n"
     ]
    }
   ],
   "source": [
    "print(len(dialogs))\n",
    "print(max(dialogs_len), min(dialogs_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dialogs(dialogs, dialogs_len, percentile=90):\n",
    "    \n",
    "    d_lens = np.array(dialogs_len)\n",
    "    reasonable_length = np.percentile(d_lens, percentile)\n",
    "    dialogs_reduced = [dialog for dialog, length in zip(dialogs, dialogs_len) if length <= reasonable_length]\n",
    "    \n",
    "    return dialogs_reduced   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dialogs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d21f2be88bad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdialogs_reduced\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_dialogs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdialogs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdialogs_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdialogs_reduced\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dialogs' is not defined"
     ]
    }
   ],
   "source": [
    "dialogs_reduced = filter_dialogs(dialogs, dialogs_len)\n",
    "print(len(dialogs_reduced)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pairs(dialogs = None, cache_file='cache_folder\\\\pairs.txt'):\n",
    "    \"\"\"\n",
    "    Function that creates pairs of input, output from dialogs, each dialogs corresponds now to many pairs.\n",
    "    \n",
    "    :param dialogs: a list with all the dialogs \n",
    "    :return a list whose elements are pairs of input, output  \n",
    "    \"\"\"\n",
    "    try: \n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            print(\"Cache file found loading content.\")\n",
    "            pairs = pickle.load(f)\n",
    "            return pairs\n",
    "    except: # cache file not created yet\n",
    "        print(\"Cache file not found. Start processing.\")\n",
    "        pairs = [] \n",
    "        for dialog in dialogs:\n",
    "            t_dict = {'input': []}\n",
    "            if len(dialog) % 2 != 0: # discard the last phrase if it was said by the user\n",
    "                dialog = dialog[:-1]\n",
    "            dialog_it = iter(dialog)\n",
    "            for i_phrase, o_phrase in zip_longest(dialog_it, dialog_it): # process phrases two by two\n",
    "                try:\n",
    "                    t_dict[\"input\"].append(t_dict[\"output\"])\n",
    "                except:\n",
    "                    pass\n",
    "                t_dict[\"input\"].append(i_phrase) # history\n",
    "                t_dict[\"output\"] = o_phrase\n",
    "                pairs.append(t_dict)\n",
    "                t_dict = copy.deepcopy(t_dict) # so future changes address only the new dict\n",
    "        with open(cache_file, \"wb\") as f:\n",
    "            pickle.dump(pairs, f)\n",
    "        return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache file found loading content.\n"
     ]
    }
   ],
   "source": [
    "pairs = extract_pairs() #dialogs_reduced list of dictionaries of two keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_history(pairs, max_history=2): # seq len reduced from 153 to 124\n",
    "    for pair in pairs:\n",
    "        pair['input'] = pair['input'][-(2*max_history+1):] # at least one phrase is preserved\n",
    "    return pairs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = adjust_history(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pairs(pairs):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15684\n",
      "{'input': [[488, 249, 1074, 12361, 15354, 504, 481, 3361], [249, 2518, 512, 1074, 246, 5358, 500, 481, 3361], [668, 5611, 239, 249, 1074, 688, 504, 2306], [525, 256, 252, 246, 1875, 4778], [912, 249, 1048, 246, 16219, 267]], 'output': [249, 2310, 256, 241, 2153, 485, 699, 512]}\n"
     ]
    }
   ],
   "source": [
    "print(len(pairs))\n",
    "print(pairs[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO να σώζεται σε αρχείο στην πιο κατάλληλη μορφή. Να δούμε αν βολεύει Pandas ή κάτι άλλο "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate a gpt pytorch model with pre-trained weights on language modelling task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "A helper class used to interact with the vocabulary in which our model has been pre-trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our language model have been pre-trained with a vocabulary of 40478 words.\n"
     ]
    }
   ],
   "source": [
    "print(\"Our language model have been pre-trained with a vocabulary of {} words.\".format(tokenizer.vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Εμείς έχουμε πάντα περιττού πλήθους history που αρχίζει και τελειώνει με speaker1 και reply που το λέει ο speaker2\n",
    "\n",
    "Για το input_ids: Η λογική είναι αναθέτει τον speaker2 κάθε φορά που μένει άρτιο πλήθος από διαλόγους(περιττό συνολικά μαζί με sos, βλέπε συνθήκη). Εμάς όλες μας οι λίστες έχουν άρτιο πλήθος οπότε θα ξεκινήσει με speaker2 ενώ θέλουμε speaker1. Τα επιμέρους αποτελεσματα όμως είναι συμβατά μεταξύ τους.\n",
    "\n",
    "Στο input_ids το i στο iter παίρνει τιμή i = seq_len - 2 (αφού ξεκινήσαμε από το δεύτερο στοιχείο το iteration)\n",
    "\n",
    "Για το token_type_ids: για κάθε μία λίστα κάνουμε iterate στα στοιχεία της, αν η θέση της λίστας είναι άρτια παίρνει speaker1 αλλιώς speaker2\n",
    "Στο token_type_ids: επειδή το πλήθος είναι περιττό με την προσθήκη του sos θα αλλάξει η σειρά και η πρώτη πρόταση θα πάει speaker2 και το reply speaker1\n",
    "\n",
    "Καταρχάς η αντιστοιχία που δίνουν οι ίδιοι στο δικό τους δεν ταιριάζει με αυτό που είχαμε σκεφτεί \n",
    "Κατά δεύτερο πρέπει να δούμε που θα μπει αν θα μπει το sos, αυτό μας δημιουργεί πρόβλημα αυτή τη στιγμή. Θα μπει μετά το tag του speaker ? \n",
    "\n",
    "είτε θα μπει μόνο του πριν τον speaker\n",
    "σε αυτή την περίπτωση θα πρέπει να παίρνει το tag του speaker1 στα tokens αυτό δε συμβαίνει τώρα και μας μπερδεύει τη σειρά \n",
    "\n",
    "για το label βάζει σε όλα τα inputs εκτός του reply -1, στο speaker2 του reply -1 και βάζει τα tokens του reply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τι ακριβώς θα δούμε με το validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_array = np.array(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13996 3500\n"
     ]
    }
   ],
   "source": [
    "pairs_train_l = pairs[:int(len(pairs)*0.8)]\n",
    "pairs_eval_l = pairs[int(len(pairs)*0.8):]\n",
    "print(len(pairs_train_l), len(pairs_eval_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Για διαχωρισμό σε train test\n",
    "υπάρχει κάτι που να δημιουργεί πρόβλημα;\n",
    "μπορεί να μας ενοχλεί ότι ζεύγη που έρχονται από διαφορετικούς διαλόγους θα χωριστούν; μας ενοχλεί αν δεν μπαίνουν με τη σειρά;\n",
    "αν δεν κάνουμε τυχαίο split κάποια domains δε θα εμφανίζονται στο train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_train, pairs_eval = train_test_split(pairs_array, test_size=0.2, shuffle=True)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12547,) (3137,)\n",
      "{'input': [[13659, 2679], [1304, 239, 718, 1272, 12286, 587, 512, 966, 507, 485, 580, 257], [277], [566, 2499, 13659, 239, 587, 512, 604, 246, 5052, 5855, 257], [13103, 3597, 246, 3592]], 'output': [1304, 886, 507, 239, 544, 655, 246, 7537, 1807, 512, 640, 1081, 491, 257]}\n",
      "wwwwwww\n",
      "[[249, 966, 485, 5838, 531, 4895]]\n"
     ]
    }
   ],
   "source": [
    "print(pairs_train.shape, pairs_eval.shape)\n",
    "print(pairs_train[0])\n",
    "print(\"wwwwwww\")\n",
    "print(pairs_eval[0]['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12547,) (3137,)\n",
      "{'input': [[249, 966, 485, 34427, 531, 4895], [881], [1462, 547, 6307, 4895, 1572], [1304, 1256, 239], [1462, 481, 43, 10458, 6307, 4895, 485, 44, 10458, 504, 5513]], 'output': [1304, 239, 636, 512, 1362, 507, 257]}\n",
      "wwwwwww\n",
      "[[759, 512, 5838, 547, 292, 1048, 4895, 504, 5498, 488, 11072, 507, 485, 1099, 2433, 850, 257], [773, 512, 823, 704, 54, 1048, 4895, 617, 5498, 822, 5375, 257], [685, 240, 525, 256, 252, 770, 239], [773, 240, 4895, 544, 1233, 239, 544, 655, 1033, 1284, 249, 759, 587, 562, 512, 257], [2548, 239, 1359, 635, 512, 1233, 481, 8358, 485, 11477, 297, 562, 525, 257]]\n"
     ]
    }
   ],
   "source": [
    "print(pairs_train.shape, pairs_eval.shape)\n",
    "print(pairs_train[0])\n",
    "print(\"wwwwwww\")\n",
    "print(pairs_eval[0]['input'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Θέλω στο μοντέλο μου να δίνω τρία inputs όπως το παράγει η συνάρτηση build, συνεπώς αυτό θέλω να μου γυρίζει η συνάρτηση get item "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO check if we can use torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dialog_pairs):\n",
    "        self.dataset = self.create_segments(dialog_pairs)\n",
    "        self.dataset = self.pad_sequenses(self.dataset, tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1])) # χρειάζεται η ανάθεση ?\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def create_segments(self, dialog_pairs):\n",
    "        dataset = []\n",
    "        for pair in dialog_pairs:\n",
    "            instance = build_input_from_segments(pair['input'], pair['output'], tokenizer)\n",
    "            dataset.append(instance)\n",
    "        return dataset\n",
    "    \n",
    "    def pad_sequenses(self, dataset, padding=0):\n",
    "        # TODO create mask for each entry\n",
    "        max_seq_len = max(len(entry[\"input_ids\"]) for entry in dataset)\n",
    "        print(max_seq_len)\n",
    "        for entry in dataset:\n",
    "            for index_name in entry.keys():\n",
    "                entry[index_name] =  entry[index_name] + [padding if index_name != \"lm_labels\" else -1] * (max_seq_len - len(entry[index_name]))\n",
    "        return dataset    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "#         index_ids = self.index_ids[index]\n",
    "#         category_ids = self.category[index]\n",
    "#         labels_ids = self.labels[index]\n",
    "        sample = self.dataset[index]\n",
    "        return (np.array(sample['input_ids']), np.array(sample['token_type_ids']), np.array(sample[\"lm_labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_from_segments(history, reply, tokenizer, with_eos=True):\n",
    "    \"\"\" Build a sequence of input from 2 segments:  history and last reply. \"\"\"\n",
    "    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
    "    sequence = [[bos]] + history + [reply + ([eos] if with_eos else [])]\n",
    "    seq_len = len(sequence) # sequence λίστα από λίστες\n",
    "    # print(seq_len)\n",
    "    sequence = [sequence[0]] + [[speaker2 if (seq_len-i) % 2 != 1 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "    # print(sequence)\n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence)) # words\n",
    "    instance[\"token_type_ids\"] = [speaker1] + [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence[1:]) for _ in s] # for each word\n",
    "    # TODO mask\n",
    "    instance[\"lm_labels\"] = ([-1] * sum(len(s) for s in sequence[:-1])) + [-1] + sequence[-1][1:]\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n",
      "124\n"
     ]
    }
   ],
   "source": [
    "training_set = DialogDataset(pairs_train) \n",
    "validation_set = DialogDataset(pairs_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 \n",
    "\"\"\"\n",
    "TODO: pad on batch level \n",
    "\n",
    "in order to avoid padding to the global max_len we can define our own collate_fn\n",
    "which forms the samples into batches and call inside there the pad function.\n",
    "Samples should be allocated to batches based on their sequence length in order to\n",
    "minimize the need for padding.\n",
    "\"\"\" \n",
    "\n",
    "dataloader_train = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0) # has shuffle any sideffects here?\n",
    "dataloader_valid = DataLoader(validation_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "min_loss, max_patience, cur_patience = np.inf, 10, 0\n",
    "save_file = \"chatbot.pkl\"\n",
    "use_cuda = False\n",
    "\n",
    "# loss_function = nn. check it την διαλέγει μόνο του?\n",
    "# αν δοθεί το labels αρχικοποιεί και χρησιμοποιεί εσωτερικά το crossEntropyLoss\n",
    "# να πούμε αναλυτικά τι κάνει σε πρώτη φάση, αν δε το γράψουμε χεράτα.\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.001) # TODO review those values\n",
    "for epoch in range(epochs):\n",
    "    # aux staff here\n",
    "    model.train()\n",
    "    for i_batch, (input_ids, category_ids, label_ids) in enumerate(dataloader_train):\n",
    "#         print(type(input_ids))\n",
    "#         print(input_ids.shape)\n",
    "#         print(category_ids.shape)\n",
    "#         print(label_ids.shape)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids.type(torch.LongTensor), token_type_ids=category_ids.type(torch.LongTensor), labels=label_ids.type(torch.LongTensor))\n",
    "        print(type(outputs))\n",
    "        loss, logits = outputs[:2]\n",
    "        optimizer.zero_grad() # model.zero_grad() and optimizer.zero_grad() are the same IF all your model parameters are in that optimizer.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        break\n",
    "        # στο loss που παίρνω μπορώ να κάνω backward απ' ότι καταλαβαίνω\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO SAVE MODEL and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction with the bot - Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(history, reply_so_far):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # print(\"History is: {}\".format(history))\n",
    "    # print(\"Reply so far is: {}\".format(reply_so_far))\n",
    "    history = [tokenizer.encode(phrase) for phrase in history]\n",
    "    \n",
    "    instance = build_input_from_segments(history, reply_so_far, tokenizer, with_eos=False)\n",
    "    \n",
    "    input_ids = torch.tensor(instance[\"input_ids\"]).unsqueeze(0)\n",
    "    token_type_ids = torch.tensor(instance[\"token_type_ids\"]).unsqueeze(0)\n",
    "    \n",
    "    return input_ids, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding(logits, method=\"greedy\"):\n",
    "    \"\"\"\n",
    "    Functions that selects the next token to be emmited. Three different approaches are implemented: \n",
    "    \n",
    "    Greedy: the most probable token is selected.\n",
    "    Top-k : \n",
    "    Top-p : \n",
    "    \n",
    "    :param logits: \n",
    "    :param method: the decoding method to be used, Values={'greedy', 'top_k', 'top_p'}\n",
    "    :return: the selected token\n",
    "    \"\"\"\n",
    "    top_k = 100 # sample from the 100 most probable tokens based on their probs\n",
    "    top_p = 0.9 # sample from the n most probable tokens that have a cumulative probability > 0.9 \n",
    "    \n",
    "    if method == \"top_k\":\n",
    "        logit_k = torch.topk(logits, top_k)[0][-1].item() # value of the 100th most probable\n",
    "        logits[logits < logit_k] = -float('Inf')   # cut off the tail  \n",
    "        \n",
    "    elif method == \"top_p\":\n",
    "        \n",
    "    # default: greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.6177,  3.1210,  2.3628,  2.2761,  2.1987,  1.5925,  1.2785,  1.2639,\n",
      "         1.1791,  0.8849,  0.8654,  0.7551,  0.6730,  0.6603,  0.5451,  0.5403,\n",
      "         0.4563,  0.3471,  0.2676,  0.1768,  0.0710,  0.0299,  0.0290,  0.0133,\n",
      "         0.0039, -0.0358, -0.0556, -0.0771, -0.1768, -0.3003, -0.3305, -0.4515,\n",
      "        -0.4820, -0.4945, -0.5143, -0.5508, -0.5748, -0.6180, -0.6758, -0.6847,\n",
      "        -0.6874, -0.6942, -0.7083, -0.7232, -0.7593, -0.7905, -0.8563, -0.9661,\n",
      "        -1.0109, -1.0398, -1.0596, -1.0603, -1.0604, -1.0776, -1.0814, -1.1205,\n",
      "        -1.1241, -1.1291, -1.1345, -1.1431, -1.1492, -1.2057, -1.2991, -1.3368,\n",
      "        -1.3590, -1.3609, -1.3838, -1.4533, -1.4547, -1.4550, -1.4569, -1.5742,\n",
      "        -1.6001, -1.6469, -1.6515, -1.6707, -1.7707, -1.7743, -1.7827, -1.8013,\n",
      "        -1.8057, -1.8740, -1.9021, -1.9383, -1.9434, -1.9744, -1.9816, -1.9875,\n",
      "        -2.0014, -2.0252, -2.0295, -2.0455, -2.0740, -2.1106, -2.1302, -2.1389,\n",
      "        -2.1672, -2.1854, -2.2004, -2.2230], grad_fn=<TopkBackward>)\n",
      "-2.2229726314544678\n",
      "-2.2229726314544678\n",
      "tensor([0.2408, 0.1466, 0.0687, 0.0630, 0.0583, 0.0318, 0.0232, 0.0229, 0.0210,\n",
      "        0.0157, 0.0154, 0.0138, 0.0127, 0.0125, 0.0112, 0.0111, 0.0102, 0.0091,\n",
      "        0.0084, 0.0077, 0.0069, 0.0067, 0.0067, 0.0066, 0.0065, 0.0062, 0.0061,\n",
      "        0.0060, 0.0054, 0.0048, 0.0046, 0.0041, 0.0040, 0.0039, 0.0039, 0.0037,\n",
      "        0.0036, 0.0035, 0.0033, 0.0033, 0.0033, 0.0032, 0.0032, 0.0031, 0.0030,\n",
      "        0.0029, 0.0027, 0.0025, 0.0024, 0.0023, 0.0022, 0.0022, 0.0022, 0.0022,\n",
      "        0.0022, 0.0021, 0.0021, 0.0021, 0.0021, 0.0021, 0.0020, 0.0019, 0.0018,\n",
      "        0.0017, 0.0017, 0.0017, 0.0016, 0.0015, 0.0015, 0.0015, 0.0015, 0.0013,\n",
      "        0.0013, 0.0012, 0.0012, 0.0012, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
      "        0.0010, 0.0010, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009, 0.0009,\n",
      "        0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0008, 0.0007, 0.0007, 0.0007,\n",
      "        0.0007], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "tst = torch.topk(logits[0, -1, :], 100)[0][..., -1, None]\n",
    "print(ts)\n",
    "print(ts[-1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_answer(history, model):\n",
    "    \"\"\"\n",
    "    Function that generates word by word the bot answer, based on user input and previous history\n",
    "    \n",
    "    :param history: a list of past sentences and last user's input, in plain text\n",
    "    :param model: the model to be used for inference\n",
    "    :return: a list with the words of the answer in plain text \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    reply_so_far = []\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i in range(max_sentence_length):\n",
    "            \n",
    "            input_ids, category_ids = format_input(history, reply_so_far)\n",
    "            # print(\"Inputs ids are {}\".format(input_ids)) seems good\n",
    "            # print(\"Category ids are {}\".format(category_ids)) seems good\n",
    "            outputs = model(input_ids=input_ids, token_type_ids=category_ids)\n",
    "            logits = outputs[0]\n",
    "            logits = logits[0, -1, :] / temperature # keep last \n",
    "            probs = F.softmax(logits, dim=-1) # greedy decoding\n",
    "            word = torch.argmax(probs).item()\n",
    "            \n",
    "            if word in SPECIAL_TOKENS_IDS: # we stop inference if we find a special token without emitting this token\n",
    "                print(\"Bot terminate sentence!\")\n",
    "                break\n",
    "            reply_so_far.append(word)\n",
    "            \n",
    "        answer_text = tokenizer.decode(reply_so_far, skip_special_tokens=True)    \n",
    "        return answer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_with_bot(model):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    bot_prompt = \"bot:>>> \"\n",
    "    user_prompt = \"user:>>> \"\n",
    "\n",
    "    history = []\n",
    "    print(bot_prompt + \"Hello how may I help you?\")\n",
    "    user_input = input(user_prompt)\n",
    "    \n",
    "    while user_input != \"\\q\":\n",
    "        # TODO truncate user input to not exceed max_length\n",
    "        history.append(user_input)\n",
    "        answer = infer_answer(history, model)\n",
    "        history.append(answer)\n",
    "        \n",
    "        history = history[-(2*max_history+1):]  # keep the same history as in the training \n",
    "        \n",
    "        print(bot_prompt + answer)\n",
    "        \n",
    "        user_input = input(user_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bot:>>> Hello how may I help you?\n",
      "user:>>> We would need such derived metrics that literature considers as indicators.\n",
      "bot:>>> \" \n",
      " \" i'm not sure i understand. \" \n",
      " \" i'm not sure i understand either\n",
      "user:>>> what's the issue here?\n",
      "bot:>>> , or what'e's the issue here. \" \n",
      " \" i'm not sure i understand.\n",
      "user:>>> \\q\n"
     ]
    }
   ],
   "source": [
    "interact_with_bot(model)\n",
    "test = \"could you book a ticket for me?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUG AREA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, logits = outputs[:2]\n",
    "print(loss)\n",
    "print(logits.shape)\n",
    "\n",
    "model.eval()\n",
    "outputs_e = model(input_ids=input_ids.type(torch.LongTensor), token_type_ids=category_ids.type(torch.LongTensor))\n",
    "print(type(outputs_e))\n",
    "\n",
    "print(len(outputs_e))\n",
    "print(outputs_e[0].shape)\n",
    "logits_e = outputs_e[0]\n",
    "logits_e = logits_e[0, -1, :] / temperature\n",
    "print(logits_e.shape)\n",
    "probs = F.softmax(logits_e, dim=-1)\n",
    "\n",
    "word = torch.argmax(probs)\n",
    "word_3 = torch.argmax(logits_e)\n",
    "# word_2 = torch.topk(probs, 1)[1]\n",
    "print(word.item()) \n",
    "print(word_3.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_segments(dialog_pairs):\n",
    "    dataset = []\n",
    "    for pair in dialog_pairs:\n",
    "        instance = build_input_from_segments(pair['input'], pair['output'], tokenizer)\n",
    "        dataset.append(instance)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def pad_sequenses(dataset, padding=0):\n",
    "    # TODO create mask for each entry\n",
    "    max_seq_len = max(len(entry[\"input_ids\"]) for entry in dataset)\n",
    "    print(max_seq_len)\n",
    "    for entry in dataset:\n",
    "        for index_name in entry.keys():\n",
    "            entry[index_name] =  entry[index_name] + [padding if index_name != \"lm_labels\" else -1] * (max_seq_len - len(entry[index_name]))\n",
    "        # entry[index_name] =  [ for index_name in entry.keys()]\n",
    "    return dataset   \n",
    "\n",
    "test_1 = create_segments(pairs_train)\n",
    "test_2 = pad_sequenses(test_1)\n",
    "test_1[0]\n",
    "test_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(40483, 768)"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(tokenizer.encoder)\n",
    "# model.resize_token_embeddings(new_num_tokens=40478 + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = OpenAIGPTDoubleHeadsModel.from_pretrained('openai-gpt')\n",
    "# print(model)\n",
    "# model_raw = OpenAIGPTModel.from_pretrained('openai-gpt')\n",
    "# print(model_raw) # without the last linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['could</w>', 'you</w>', 'book</w>', 'me</w>', 'a</w>', 'ticket</w>', '?</w>']\n",
      "[635, 512, 1861, 510, 246, 8194, 257]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249, 249]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "it = iter(a)\n",
    "testt = [it]*3\n",
    "\n",
    "test_tok = tokenizer.tokenize('could you book me a ticket?')\n",
    "print(test_tok)\n",
    "print(tokenizer.convert_tokens_to_ids(test_tok)) # all tokens must be lowercase \n",
    "tokenizer.encode('i i i i i i i i i i i i i i i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_len # what does it means ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      "token_type_ids\n",
      "lm_labels\n"
     ]
    }
   ],
   "source": [
    "for index_name in instance.keys():\n",
    "    print(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug cell\n",
    "def build_inputs(persona, history, reply):\n",
    "    # Build our sequence by adding delimiters and concatenating\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + [eos]]\n",
    "    sequence = [sequence[0]] + [ [speaker2 if (len(sequence)-i) % 2 else speaker1] + s\n",
    "                                for i, s in enumerate(sequence[1:])]\n",
    "    # Build our word, segments and position inputs from the sequence\n",
    "    words = list(chain(*sequence))                          # word tokens\n",
    "    segments = [speaker2 if i % 2 else speaker1             # segment tokens\n",
    "                for i, s in enumerate(sequence) for _ in s]\n",
    "    position = list(range(len(words)))                      # position tokens\n",
    "    return words, segments, position, sequence\n",
    "\n",
    "persona = [[\"i\", \"like\", \"playing\", \"football\", \".\"],\n",
    "           [\"i\", \"am\", \"from\", \"NYC\", \".\"]]\n",
    "history = [[\"hello\", \"how\", \"are\", \"you\", \"?\"],\n",
    "           [\"i\", \"am\", \"fine\", \"thanks\", \".\"]]\n",
    "reply = [\"great\", \"to\", \"hear\"]\n",
    "\n",
    "sequence = [[\"<bos>\"] + list(chain(*persona))] + history +  [reply + [\"<eos>\"]]\n",
    "sequence = [sequence[0]] + [ [speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "print(sequence)\n",
    "print(list(chain(*sequence))   )\n",
    "[speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
    "\n",
    "def build_input_from_segments_or(persona, history, reply, tokenizer, lm_labels=False, with_eos=True):\n",
    "    \"\"\" Build a sequence of input from 3 segments: persona, history and last reply. \"\"\"\n",
    "    bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1])\n",
    "    sequence = [[bos] + list(chain(*persona))] + history + [reply + ([eos] if with_eos else [])]  # chain: expands the lists  \n",
    "    sequence = [sequence[0]] + [[speaker2 if (len(sequence)-i) % 2 else speaker1] + s for i, s in enumerate(sequence[1:])]\n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence)) # words\n",
    "    instance[\"token_type_ids\"] = [speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s]\n",
    "    instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
    "    instance[\"lm_labels\"] = [-1] * len(instance[\"input_ids\"])\n",
    "    if lm_labels:\n",
    "        instance[\"lm_labels\"] = ([-1] * sum(len(s) for s in sequence[:-1])) + [-1] + sequence[-1][1:]\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[[40478], [40481, 249, 1048, 6702], [40482, 498, 1385, 512, 640], [40481, 488, 249, 1074, 12361, 15354, 504, 481, 3361], [40482, 249, 2518, 512, 1074, 246, 5358, 500, 481, 3361, 40479]]\n"
     ]
    }
   ],
   "source": [
    "# debug cell\n",
    "history = [[249, 1048, 6702], [498, 1385, 512, 640], [488, 249, 1074, 12361, 15354, 504, 481, 3361]]\n",
    "reply =  [249, 2518, 512, 1074, 246, 5358, 500, 481, 3361]\n",
    "\n",
    "instance = build_input_from_segments(history, reply, tokenizer)\n",
    "instance_or = build_input_from_segments_or(persona, history, reply, tokenizer, lm_labels=True)\n",
    "\n",
    "print(tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[:-1]))\n",
    "instance_or[\"input_ids\"]\n",
    "instance_or[\"token_type_ids\"]\n",
    "instance[\"input_ids\"] \n",
    "instance[\"token_type_ids\"] \n",
    "instance = build_input_from_segments(persona, history, reply, tokenizer)\n",
    "\n",
    "### ασυμβίβαστα μεταξύ τους στο input_ids βγαίνει ότι το reply το είπε ο speaker1, ενώ στο token_type_ids ότι το είπε ο speaker2 (μάλλον για τον κώδικα του medium μόνο)\n",
    "\n",
    "lm_targets = ([-1] * sum(len(s) for s in sequence[:-1])) \\\n",
    "             + [-1] + tokenizer.convert_tokens_to_ids(sequence[-1][1:])\n",
    "\n",
    "lm_targets # στα labels tou language model έχουν τιμές μόνο τα tokens του reply.\n",
    "lm_distractor = [-1] * len(instance[\"input_ids\"])\n",
    "lm_distractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 249,\n",
       " 2518,\n",
       " 512,\n",
       " 1074,\n",
       " 246,\n",
       " 5358,\n",
       " 500,\n",
       " 481,\n",
       " 3361,\n",
       " 40479]"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance['lm_labels']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
