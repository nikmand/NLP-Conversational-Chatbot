{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: Create a Chatbot\n",
    "\n",
    "### Name: Nikiforos Mandilaras\n",
    "\n",
    "### Email: nikiforosmandi@windowslive.com\n",
    "\n",
    "### Date: 15/11/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "import copy\n",
    "import statistics\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "from transformers import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer, WEIGHTS_NAME, CONFIG_NAME\n",
    "from itertools import chain\n",
    "from ast import literal_eval\n",
    "from itertools import zip_longest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(output_dir='openai-gpt'):\n",
    "    \"\"\"\n",
    "    Loads GPT Model and the corresponding tokenizer from local checkpoint.\n",
    "    If no path is specified the pretrained weights on language modelling task are downloaded.\n",
    "    \n",
    "    :param output_dir: path to checkpoint \n",
    "    :return : model and tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = OpenAIGPTTokenizer.from_pretrained(output_dir)\n",
    "    model = OpenAIGPTLMHeadModel.from_pretrained(output_dir)  \n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_checkpoint() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable model parameters: 116538624\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Number of trainable model parameters: {}\".format(trainable_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO check number of parameters\n",
    "# print(model) # check the architecture of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTR_TO_SPECIAL_TOKEN = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
    "                         'additional_special_tokens': ('<speaker1>', '<speaker2>')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens_(model, tokenizer):\n",
    "    \"\"\"\n",
    "    Adds special tokens to the tokenizer and if they weren't present to the model also.\n",
    "    \"\"\"\n",
    "    orig_num_tokens = len(tokenizer.encoder)\n",
    "    num_added_tokens = tokenizer.add_special_tokens(ATTR_TO_SPECIAL_TOKEN) # doesn't add if they are already there\n",
    "    \n",
    "    if num_added_tokens > 0:\n",
    "        print(\"New tokens added to model: {}\".format(num_added_tokens))\n",
    "        model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens) \n",
    "    else:\n",
    "        print(\"No new tokens found! Nothing added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New tokens added to model: 5\n"
     ]
    }
   ],
   "source": [
    "# various constants needed\n",
    "\n",
    "max_history = 2 # pairs of question/answer to be retained\n",
    "min_sentence_length = 1\n",
    "max_sentence_length = 20 # maximum length of a sentence produced by the model \n",
    "\n",
    "temperature = 0.75 # increases confidence in the most propable outputs \n",
    "\n",
    "use_cuda = False # whether to try to use cuda or not\n",
    "\n",
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
    "add_special_tokens_(model, tokenizer)  \n",
    "\n",
    "SPECIAL_TOKENS_IDS = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
    "TIME_FORMAT = '%Y%m%d_%H%M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40478, 40479, 40481, 40482, 40480]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPECIAL_TOKENS_IDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = 'C:\\\\Users\\\\nikmand\\\\nikmand\\\\ncsr-chatbot\\\\'  # os.getcwd()\n",
    "\n",
    "# TODO new function for the tokenization process\n",
    "\n",
    "def parser(datafolder='metalwoz-v1/dialoguesTest/', cache_file='cache_folder/dialogs.txt'): \n",
    "    \"\"\"\n",
    "    Function that reads files, keeps only 'turns' from each entry and tokenizes them\n",
    "\n",
    "    :param datafolder: path to the folder that contains the files\n",
    "    :return: a list that contains dialogs, each dialog is a list of lists \n",
    "             where each of them represents the ids of a phrase,\n",
    "    \"\"\"\n",
    "    try: \n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            print(\"Cache file found loading content.\")\n",
    "            dialogs = pickle.load(f)\n",
    "            return dialogs\n",
    "    except: # cache file not created yet\n",
    "         print(\"Cache file not found. Start processing.\")       \n",
    "    \n",
    "        dialogs, dialogs_len = [], []\n",
    "        files = list(glob.glob(os.path.join(workspace, datafolder ,\"*.txt\")))\n",
    "\n",
    "        for file in files:\n",
    "            with open(file) as f:\n",
    "                for line in f.readlines():\n",
    "\n",
    "                    dialog = literal_eval(line)['turns'][1:] # keep only turns without the first sentence\n",
    "                    dialog = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(phrase)) for phrase in dialog]  \n",
    "                    dialogs.append(dialog) \n",
    "\n",
    "        with open(cache_file, \"wb\") as f:\n",
    "            pickle.dump(dialogs, f)            \n",
    "                \n",
    "    return dialogs      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pairs(dialogs = None, cache_file='cache_folder/pairs.txt'):\n",
    "    \"\"\"\n",
    "    Function that creates pairs of input, output from dialogs, each dialogs corresponds now to many pairs.\n",
    "    \n",
    "    :param dialogs: a list with all the dialogs \n",
    "    :param cache_file: path to save the result\n",
    "    \n",
    "    :return : a list whose elements are pairs of input(history), output(expected bot reply)  \n",
    "    \"\"\"\n",
    "    try:         # try to open cache file\n",
    "        \n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            print(\"Cache file found loading content.\")\n",
    "            pairs = pickle.load(f)       \n",
    "            return pairs\n",
    "        \n",
    "    except:      # cache file not created yet\n",
    "        print(\"Cache file not found. Start processing.\")\n",
    "        \n",
    "        pairs = [] \n",
    "        for dialog in dialogs:\n",
    "            \n",
    "            t_dict = {'input': []}\n",
    "            if len(dialog) % 2 != 0: # discard the last phrase if it was said by the user\n",
    "                dialog = dialog[:-1]    \n",
    "            dialog_it = iter(dialog)\n",
    "            \n",
    "            for i_phrase, o_phrase in zip_longest(dialog_it, dialog_it): # process phrases two by two        \n",
    "                try:\n",
    "                    t_dict[\"input\"].append(t_dict[\"output\"])\n",
    "                except:\n",
    "                    pass \n",
    "                t_dict[\"input\"].append(i_phrase) # history\n",
    "                t_dict[\"output\"] = o_phrase\n",
    "                pairs.append(t_dict)\n",
    "                t_dict = copy.deepcopy(t_dict) # so future changes address only the new dict\n",
    "                \n",
    "        with open(cache_file, \"wb\") as f:  # save result so future calls can retrieve it right away\n",
    "            pickle.dump(pairs, f)\n",
    "            \n",
    "        return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_history(pairs, max_history=2): # seq len reduced from 263 to 181\n",
    "    \"\"\"\n",
    "    Reduces number of previous chat senteces that are going to be included in the input\n",
    "    \n",
    "    :param pairs: list with samples \n",
    "    :param max_history: Number of pairs of question/answer to be preserved (at least one is preserved)\n",
    "    return : two lists, pairs with fixed history and theirs corresponding seq_lenghts \n",
    "    \"\"\"\n",
    "    \n",
    "    pairs_len = []\n",
    "    for pair in pairs:\n",
    "        \n",
    "        pair['input'] = pair['input'][-(2*max_history+1):] # at least one phrase is preserved\n",
    "        pair_len = sum(len(phrase) for phrase in pair['input']) + len(pair['output'])\n",
    "        pairs_len.append(pair_len)\n",
    "        \n",
    "    return pairs, pairs_len   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_samples(samples, samples_len, percentile=90):\n",
    "    \"\"\"\n",
    "    Filters samples based on sequence lengths.  \n",
    "    \n",
    "    :param samples: a list with samples\n",
    "    :param samples_len: their corresponding lengths\n",
    "    :param percentile: percentage of samples to preserve  \n",
    "    \n",
    "    :return : two lists, preserved samples and their lengths\n",
    "    \"\"\"\n",
    "    \n",
    "    samples_length = np.array(samples_len)\n",
    "    reasonable_length = np.percentile(samples_length, percentile)\n",
    "    print(\"{}% of the samples have sequence length less than {}\".format(percentile, reasonable_length))\n",
    "    \n",
    "    samples_red, samples_len_red = [], []\n",
    "    for sample, sample_len in zip(samples, samples_len):\n",
    "        \n",
    "        if sample_len <= reasonable_length:\n",
    "            samples_red.append(sample)\n",
    "            samples_len_red.append(sample_len)\n",
    "    \n",
    "    return samples_red, samples_len_red  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dialogs in the whole dataset: 3828\n",
      "Cache file found loading content.\n",
      "Number of pairs created: 19379\n",
      "Maximum seq_length observed: 181\n",
      "90% of the samples have sequence length less than 81.0\n"
     ]
    }
   ],
   "source": [
    "dialogs = parser()\n",
    "\n",
    "print(\"Number of dialogs in the whole dataset: {}\".format(len(dialogs)))\n",
    "\n",
    "pairs = extract_pairs(dialogs) # list of dictionaries of input history and bot's reply\n",
    "\n",
    "# keep only portion of the chat history to reduce seq_length\n",
    "pairs, pairs_len = adjust_history(pairs, max_history=max_history) \n",
    "\n",
    "print(\"Number of pairs created: {}\".format(len(pairs)))\n",
    "print(\"Maximum seq_length observed: {}\".format(max(pairs_len)))\n",
    "\n",
    "pairs_reduced, pairs_len_reduced = filter_samples(pairs, pairs_len) # reduces from 181 to 81 (history 2) or from 263 to 108\n",
    "# mean leangth with history 2 is  47 and max 181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate a gpt pytorch model with pre-trained weights on language modelling task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "A helper class used to interact with the vocabulary in which our model has been pre-trained.\n",
    "Our language model have been pre-trained with a vocabulary of {} words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our language model have been pre-trained with a vocabulary of 40478 words.\n"
     ]
    }
   ],
   "source": [
    "print(\"Our language model have been pre-trained with a vocabulary of {} words.\".format(tokenizer.vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Εμείς έχουμε πάντα περιττού πλήθους history που αρχίζει και τελειώνει με speaker1 και reply που το λέει ο speaker2\n",
    "\n",
    "Για το input_ids: Η λογική είναι αναθέτει τον speaker2 κάθε φορά που μένει άρτιο πλήθος από διαλόγους(περιττό συνολικά μαζί με sos, βλέπε συνθήκη). Εμάς όλες μας οι λίστες έχουν άρτιο πλήθος οπότε θα ξεκινήσει με speaker2 ενώ θέλουμε speaker1. Τα επιμέρους αποτελεσματα όμως είναι συμβατά μεταξύ τους.\n",
    "\n",
    "Στο input_ids το i στο iter παίρνει τιμή i = seq_len - 2 (αφού ξεκινήσαμε από το δεύτερο στοιχείο το iteration)\n",
    "\n",
    "Για το token_type_ids: για κάθε μία λίστα κάνουμε iterate στα στοιχεία της, αν η θέση της λίστας είναι άρτια παίρνει speaker1 αλλιώς speaker2\n",
    "Στο token_type_ids: επειδή το πλήθος είναι περιττό με την προσθήκη του sos θα αλλάξει η σειρά και η πρώτη πρόταση θα πάει speaker2 και το reply speaker1\n",
    "\n",
    "Καταρχάς η αντιστοιχία που δίνουν οι ίδιοι στο δικό τους δεν ταιριάζει με αυτό που είχαμε σκεφτεί \n",
    "Κατά δεύτερο πρέπει να δούμε που θα μπει αν θα μπει το sos, αυτό μας δημιουργεί πρόβλημα αυτή τη στιγμή. Θα μπει μετά το tag του speaker ? \n",
    "\n",
    "είτε θα μπει μόνο του πριν τον speaker\n",
    "σε αυτή την περίπτωση θα πρέπει να παίρνει το tag του speaker1 στα tokens αυτό δε συμβαίνει τώρα και μας μπερδεύει τη σειρά \n",
    "\n",
    "για το label βάζει σε όλα τα inputs εκτός του reply -1, στο speaker2 του reply -1 και βάζει τα tokens του reply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τι ακριβώς θα δούμε με το validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Για διαχωρισμό σε train test\n",
    "υπάρχει κάτι που να δημιουργεί πρόβλημα;\n",
    "μπορεί να μας ενοχλεί ότι ζεύγη που έρχονται από διαφορετικούς διαλόγους θα χωριστούν; μας ενοχλεί αν δεν μπαίνουν με τη σειρά;\n",
    "αν δεν κάνουμε τυχαίο split κάποια domains δε θα εμφανίζονται στο train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train set: 12243 and in validation: 5248\n"
     ]
    }
   ],
   "source": [
    "pairs_train, pairs_eval = train_test_split(pairs_reduced, test_size=0.3, shuffle=True)      \n",
    "\n",
    "print(\"Number of samples in train set: {} and in validation: {}\".format(len(pairs_train), len(pairs_eval)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "pad on batch level \n",
    "\n",
    "in order to avoid padding to the global max_len we can define our own collate_fn\n",
    "which forms the samples into batches and call inside there the pad function.\n",
    "Samples should be allocated to batches based on their sequence length in order to\n",
    "minimize the need for padding.\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs(history, reply, tokenizer, with_eos=True):\n",
    "    \"\"\"\n",
    "    Function that creates the various parts of the model input from input/output pairs.\n",
    "    \"\"\"\n",
    "    \n",
    "    bos, eos, speaker1, speaker2 = SPECIAL_TOKENS_IDS[:-1]\n",
    "    sequence = [[bos]] + history + [reply + ([eos] if with_eos else [])]\n",
    "    seq_len = len(sequence) # sequence: list of lists\n",
    "    sequence = [sequence[0]] + [[speaker2 if (seq_len-i) % 2 != 1 else speaker1] + s \n",
    "                                for i, s in enumerate(sequence[1:])]\n",
    "    \n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence)) # words\n",
    "    instance[\"token_type_ids\"] = [speaker1] + [speaker2 if i % 2 else speaker1 \n",
    "                                               for i, s in enumerate(sequence[1:]) for _ in s] # for each word\n",
    "    instance[\"mask\"] = [1] * len(instance[\"input_ids\"]) \n",
    "    instance[\"lm_labels\"] = ([-1] * sum(len(s) for s in sequence[:-1])) + [-1] + sequence[-1][1:]\n",
    "    # TODO positional embeddings\n",
    "    \n",
    "    return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogDataset(Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dialog_pairs):\n",
    "        self.dataset = self.create_segments(dialog_pairs)\n",
    "        self.dataset = self.sort_on_seq_length()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def create_segments(self, dialog_pairs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        dataset = []\n",
    "        for pair in dialog_pairs:\n",
    "            instance = create_model_inputs(pair['input'], pair['output'], tokenizer)\n",
    "            dataset.append(instance)\n",
    "        return dataset\n",
    "    \n",
    "    def sort_on_seq_length(self):\n",
    "        \"\"\"\n",
    "        Sorts dataset based on seq_len to minimize padding afterwards\n",
    "        \"\"\"\n",
    "        return sorted(self.dataset, key=lambda x: len(x['input_ids']))\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return  self.dataset[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequenses(batch, pad_token=0):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    max_seq_len = max(len(entry[\"input_ids\"]) for entry in batch)\n",
    "    for entry in batch:\n",
    "        for index_name in entry.keys():\n",
    "            if index_name == \"lm_labels\":\n",
    "                pad_token_ = -1\n",
    "            elif index_name == \"mask\":\n",
    "                pad_token_ = 0\n",
    "            else:\n",
    "                pad_token_ = pad_token\n",
    "            entry[index_name] =  entry[index_name] + [pad_token_] * (max_seq_len - len(entry[index_name]))\n",
    "  \n",
    "    return batch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    batch = pad_sequenses(batch, tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\n",
    "    \n",
    "    inputs = [torch.stack(list(map(lambda x: torch.from_numpy( \\\n",
    "        np.array(x[index_name])), batch)), dim=0) for index_name in batch[0].keys()]\n",
    "\n",
    "    inputs = [input_tensor.type(torch.LongTensor) for input_tensor in inputs]\n",
    "\n",
    "    if use_cuda and torch.cuda.is_available:\n",
    "        inputs = [input_tensor.cuda() for input_tensor in inputs]\n",
    "        \n",
    "    # input_ids, mask, category_ids, label_ids = inputs    \n",
    "\n",
    "    return inputs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = DialogDataset(pairs_train[:10]) \n",
    "validation_set = DialogDataset(pairs_eval[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 32 \n",
    "EVAL_BATCH_SIZE = 64 \n",
    "\n",
    "dataloader_train = DataLoader(training_set, batch_size=TRAIN_BATCH_SIZE, shuffle=False, \n",
    "                              collate_fn=custom_collate_fn, num_workers=0) \n",
    "\n",
    "dataloader_valid = DataLoader(validation_set, batch_size=EVAL_BATCH_SIZE, shuffle=False,\n",
    "                              collate_fn=custom_collate_fn, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure\n",
    "\n",
    "loss_function = nn. check it την διαλέγει μόνο του?\n",
    "αν δοθεί το labels αρχικοποιεί και χρησιμοποιεί εσωτερικά το crossEntropyLoss\n",
    "να πούμε αναλυτικά τι κάνει σε πρώτη φάση, αν δε το γράψουμε χεράτα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, print_period=400):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    epoch_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for i_batch, (input_ids, attention_mask, category_ids, label_ids) in enumerate(dataloader):\n",
    "        \n",
    "        loss, logits = model(input_ids, attention_mask, category_ids, labels=label_ids)\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()  \n",
    "        \n",
    "    return epoch_loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    epoch_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i_batch, (input_ids, attention_mask, category_ids, label_ids) in enumerate(dataloader):\n",
    "            \n",
    "            loss, logits = model(input_ids, attention_mask, category_ids, labels=label_ids)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint_state(model, tokenizer, output_dir=None):\n",
    "    \"\"\"\n",
    "    Function that saves models weights and configuration as well as tokenizer's voc.\n",
    "    \n",
    "    :param model: to checkpoint\n",
    "    :param tokenizer: to checkpoint\n",
    "    :param output_dir: directory where checkpointed files will be created\n",
    "    :retun :\n",
    "    \"\"\"\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = workspace + 'metalwoz-v1\\\\checkpoint_{}'.format(datetime.now().strftime(TIME_FORMAT))\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(output_dir)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "    output_config_file = os.path.join(output_dir, CONFIG_NAME)  \n",
    "    \n",
    "    torch.save(model.state_dict(), output_model_file) # checkpoint weights\n",
    "    model.config.to_json_file(output_config_file)     # configuration\n",
    "    tokenizer.save_vocabulary(output_dir)             # vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingException(Exception):\n",
    "    def __init__(self, message):\n",
    "\n",
    "        super().__init__(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping(min_loss, cur_patience, max_patience, epoch_eval_loss):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if (epoch_eval_loss >= min_loss):  \n",
    "        cur_patience += 1\n",
    "        if (cur_patience >= max_patience):\n",
    "            raise EarlyStoppingException(\"Execution terminated due to Early Stopping\")\n",
    "    else:\n",
    "        print(\"New min validation loss: \\t epoch {} : {:.4f}\".format(epoch, epoch_eval_loss))\n",
    "        checkpoint_state(model, tokenizer) # checkpointing\n",
    "        print(\"New checkpoint created\")\n",
    "        min_loss, cur_patience = epoch_eval_loss, 0    \n",
    "        \n",
    "    return min_loss, cur_patience    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "Loss on train set: \t\t epoch 0 : 5.0405\n",
      "88\n",
      "Loss on validation set: \t epoch 0 : 6.1711\n",
      "New min validation loss: \t epoch 0 : 6.1711\n",
      "New checkpoint created\n",
      "76\n",
      "Loss on train set: \t\t epoch 1 : 4.8323\n",
      "88\n",
      "Loss on validation set: \t epoch 1 : 6.0714\n",
      "New min validation loss: \t epoch 1 : 6.0714\n",
      "New checkpoint created\n",
      "76\n",
      "Loss on train set: \t\t epoch 2 : 4.5554\n",
      "88\n",
      "Loss on validation set: \t epoch 2 : 6.2055\n",
      "76\n",
      "Loss on train set: \t\t epoch 3 : 4.4879\n",
      "88\n",
      "Loss on validation set: \t epoch 3 : 6.0672\n",
      "New min validation loss: \t epoch 3 : 6.0672\n",
      "New checkpoint created\n",
      "76\n",
      "Loss on train set: \t\t epoch 4 : 4.3195\n",
      "88\n",
      "Loss on validation set: \t epoch 4 : 6.1503\n",
      "76\n",
      "Loss on train set: \t\t epoch 5 : 4.2222\n",
      "88\n",
      "Loss on validation set: \t epoch 5 : 6.3049\n",
      "76\n",
      "Loss on train set: \t\t epoch 6 : 4.0674\n",
      "88\n",
      "Loss on validation set: \t epoch 6 : 6.4207\n",
      "Execution terminated due to Early Stopping at epoch: 6\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 6.25e-5\n",
    "min_loss, max_patience, cur_patience = np.inf, 3, 0\n",
    "\n",
    "if use_cuda and torch.cuda.is_available:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr) # , weight_decay=0.001 # TODO review those values\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train_loss = train(model, dataloader_train, optimizer) / len(dataloader_train)\n",
    "    print(\"Loss on train set: \\t\\t epoch {} : {:.4f}\".format(epoch, train_loss))\n",
    "    \n",
    "    eval_loss = evaluate(model, dataloader_valid) / len(dataloader_valid)     \n",
    "    print(\"Loss on validation set: \\t epoch {} : {:.4f}\".format(epoch, eval_loss))    \n",
    " \n",
    "    try:\n",
    "        min_loss, cur_patience = early_stopping(min_loss, cur_patience, max_patience, eval_loss) \n",
    "    except EarlyStoppingException as e:\n",
    "        print(\"{} at epoch: {}\".format(e, epoch))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction with the bot - Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(history, reply_so_far):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    history = [tokenizer.encode(phrase) for phrase in history]\n",
    "    \n",
    "    instance = create_model_inputs(history, reply_so_far, tokenizer, with_eos=False)\n",
    "    \n",
    "    input_ids = torch.tensor(instance[\"input_ids\"]).unsqueeze(0)\n",
    "    token_type_ids = torch.tensor(instance[\"token_type_ids\"]).unsqueeze(0)\n",
    "    \n",
    "    return input_ids, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding(probs, logits, method=\"top_p\"):\n",
    "    \"\"\"\n",
    "    Functions that selects the next token to be emmited. Three different approaches are implemented: \n",
    "    \n",
    "    Greedy: the most probable token is selected.\n",
    "    Top-k : \n",
    "    Top-p : \n",
    "    \n",
    "    :param logits: \n",
    "    :param method: the decoding method to be used, Values={'greedy', 'top_k', 'top_p'}\n",
    "    :return: the selected token\n",
    "    \"\"\"\n",
    "    top_k = 40 # sample from the 100 most probable tokens based on their probs\n",
    "    top_p = 0.9 # sample from the n most probable tokens that have a cumulative probability at least 0.9 \n",
    "    \n",
    "    if method == \"greedy\":\n",
    "        return torch.argmax(probs).item()\n",
    "    \n",
    "    elif method == \"top_k\":        \n",
    "        prob_k = probs.topk(top_k)[0][-1].item() # value of the 100th most probable\n",
    "        probs[probs < prob_k] = 0   # cut off the tail  \n",
    "        \n",
    "    elif method == \"top_p\":\n",
    "        probs_sorted, probs_indexes = probs.sort(dim=-1, descending=True) # start the cumulation from the most probable token in descending order\n",
    "        cum_probs = probs_sorted.cumsum(dim=-1)\n",
    "        \n",
    "        indices = cum_probs > top_p \n",
    "        indices[1:] = indices[:-1].clone()\n",
    "        indices[0] = 0 # at least one token is preserved \n",
    "        \n",
    "        probs[probs_indexes[indices]] = 0\n",
    "    \n",
    "    word = torch.multinomial(probs, 1).item()\n",
    "    # TODO handle the case that special token was emitted in the first pick\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_answer(history, model, tokenizer, method=\"top_p\"):\n",
    "    \"\"\"\n",
    "    Function that generates word by word the bot answer, based on user input and previous history.\n",
    "    \n",
    "    :param history: a list of past sentences and last user's input, in plain text\n",
    "    :param model: the model to be used for inference\n",
    "    :return: a list with the words of the answer in plain text \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    reply_so_far = []\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i in range(max_sentence_length):\n",
    "            \n",
    "            input_ids, category_ids = format_input(history, reply_so_far)\n",
    "            # print(\"Inputs ids are {}\".format(input_ids)) seems good\n",
    "            # print(\"Category ids are {}\".format(category_ids)) seems good\n",
    "            outputs = model(input_ids=input_ids, token_type_ids=category_ids)\n",
    "            logits = outputs[0]\n",
    "            logits = logits[0, -1, :] / temperature # keep last \n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            word = decoding(probs, logits, method=method) \n",
    "            \n",
    "            if word in SPECIAL_TOKENS_IDS: # we stop inference if we find a special token without emitting this token\n",
    "                print(\"Bot terminate sentence!\")\n",
    "                break\n",
    "            reply_so_far.append(word)\n",
    "            \n",
    "        answer_text = tokenizer.decode(reply_so_far, skip_special_tokens=True)    \n",
    "        return answer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_with_bot(model, tokenizer, method='top_p'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    bot_prompt = \"bot:>>> \"\n",
    "    user_prompt = \"user:>>> \"\n",
    "\n",
    "    history = []\n",
    "    print(bot_prompt + \"Hello how may I help you?\")\n",
    "    user_input = input(user_prompt)\n",
    "    \n",
    "    while user_input != \"\\q\": # TODO check if we need to truncate user input to not exceed max_length\n",
    "        \n",
    "        history.append(user_input)\n",
    "        answer = infer_answer(history, model, tokenizer, method=method)\n",
    "        history.append(answer)\n",
    "        \n",
    "        history = history[-(2*max_history+1):]  # keep the same history as in the training \n",
    "        \n",
    "        print(bot_prompt + answer)\n",
    "        \n",
    "        user_input = input(user_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikmand\\nikmand\\ncsr-chatbot\\metalwoz-v1\\checkpoint_20191114_1250\n"
     ]
    }
   ],
   "source": [
    "model_loaded ,tokenizer_loaded  = load_checkpoint(workspace + \"metalwoz-v1\\\\checkpoint_20191114_1250\")\n",
    "\n",
    "add_special_tokens_(model_loaded, tokenizer_loaded)\n",
    "\n",
    "interact_with_bot(model_loaded, tokenizer_loaded, method='top_p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bot:>>> Hello how may I help you?\n",
      "user:>>> i need toothpaste\n",
      "Bot terminate sentence!\n",
      "bot:>>> \n",
      "user:>>> hey how are you?\n",
      "Bot terminate sentence!\n",
      "bot:>>> yes\n",
      "user:>>> tell me something.\n",
      "Bot terminate sentence!\n",
      "bot:>>> yes\n",
      "user:>>> \\q\n"
     ]
    }
   ],
   "source": [
    "test = \"could you book a ticket for me?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40478, 40479, 40481, 40482, 40480]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_loaded.convert_tokens_to_ids(SPECIAL_TOKENS) # just to check"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
