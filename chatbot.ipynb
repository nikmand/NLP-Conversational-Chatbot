{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: Create a Chatbot\n",
    "\n",
    "### Name: Nikiforos Mandilaras\n",
    "\n",
    "### Email: nikiforosmandi@windowslive.com\n",
    "\n",
    "### Date: 15/11/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This Notebook was developed as an assignment for the needs of National Center of Scientific Research Demokritos.\n",
    "The goal is to create a chatbot, a conversatonal model that can give responses to the user's inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency packaging and Execution\n",
    "\n",
    "In order to be able to execute this jupyter notebook, all depedent packages must be present. For this reason we save those along with their versions in requirements.txt (which is included in the submission folder). \n",
    "\n",
    "Then we can install them all at once using pip as it can be seen below. To avoid any collisions with existing packages it is necessary to create a new python virtual enviroment and run the command inside there. \n",
    "\n",
    "To access data the following code uses relative paths, assuming data are present in the same folder as the jupyter notebook. However all functions can accept absolute paths as well.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: attrs==19.3.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 1)) (19.3.0)\n",
      "Requirement already satisfied: backcall==0.1.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 2)) (0.1.0)\n",
      "Requirement already satisfied: bleach==3.1.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: blis==0.4.1 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 4)) (0.4.1)\n",
      "Requirement already satisfied: boto3==1.10.14 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 5)) (1.10.14)\n",
      "Requirement already satisfied: botocore==1.13.14 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 6)) (1.13.14)\n",
      "Requirement already satisfied: certifi==2019.9.11 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 7)) (2019.9.11)\n",
      "Requirement already satisfied: cffi==1.13.1 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 8)) (1.13.1)\n",
      "Requirement already satisfied: chardet==3.0.4 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 9)) (3.0.4)\n",
      "Requirement already satisfied: Click==7.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 10)) (7.0)\n",
      "Requirement already satisfied: colorama==0.4.1 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 11)) (0.4.1)\n",
      "Requirement already satisfied: cymem==2.0.2 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 12)) (2.0.2)\n",
      "Requirement already satisfied: decorator==4.4.1 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 13)) (4.4.1)\n",
      "Requirement already satisfied: defusedxml==0.6.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 14)) (0.6.0)\n",
      "Requirement already satisfied: docutils==0.15.2 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 15)) (0.15.2)\n",
      "Requirement already satisfied: entrypoints==0.3 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 16)) (0.3)\n",
      "Requirement already satisfied: idna==2.8 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 17)) (2.8)\n",
      "Requirement already satisfied: importlib-metadata==0.23 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 18)) (0.23)\n",
      "Requirement already satisfied: ipykernel==5.1.3 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 19)) (5.1.3)\n",
      "Requirement already satisfied: ipython==7.9.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 20)) (7.9.0)\n",
      "Requirement already satisfied: ipython-genutils==0.2.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 21)) (0.2.0)\n",
      "Requirement already satisfied: ipywidgets==7.5.1 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 22)) (7.5.1)\n",
      "Requirement already satisfied: jedi==0.15.1 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 23)) (0.15.1)\n",
      "Requirement already satisfied: Jinja2==2.10.3 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 24)) (2.10.3)\n",
      "Requirement already satisfied: jmespath==0.9.4 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 25)) (0.9.4)\n",
      "Requirement already satisfied: joblib==0.14.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 26)) (0.14.0)\n",
      "Requirement already satisfied: json5==0.8.5 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 27)) (0.8.5)\n",
      "Requirement already satisfied: jsonschema==3.1.1 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 28)) (3.1.1)\n",
      "Requirement already satisfied: jupyter==1.0.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 29)) (1.0.0)\n",
      "Requirement already satisfied: jupyter-client==5.3.4 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 30)) (5.3.4)\n",
      "Requirement already satisfied: jupyter-console==6.0.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 31)) (6.0.0)\n",
      "Requirement already satisfied: jupyter-core==4.6.1 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 32)) (4.6.1)\n",
      "Requirement already satisfied: jupyterlab==1.2.2 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 33)) (1.2.2)\n",
      "Requirement already satisfied: jupyterlab-server==1.0.6 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 34)) (1.0.6)\n",
      "Requirement already satisfied: MarkupSafe==1.1.1 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 35)) (1.1.1)\n",
      "Requirement already satisfied: mistune==0.8.4 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 36)) (0.8.4)\n",
      "Requirement already satisfied: mkl-fft==1.0.15 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 37)) (1.0.15)\n",
      "Requirement already satisfied: mkl-random==1.1.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 38)) (1.1.0)\n",
      "Requirement already satisfied: mkl-service==2.3.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 39)) (2.3.0)\n",
      "Requirement already satisfied: more-itertools==7.2.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 40)) (7.2.0)\n",
      "Requirement already satisfied: murmurhash==1.0.2 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 41)) (1.0.2)\n",
      "Requirement already satisfied: nbconvert==5.6.1 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 42)) (5.6.1)\n",
      "Requirement already satisfied: nbformat==4.4.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 43)) (4.4.0)\n",
      "Requirement already satisfied: notebook==6.0.2 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 44)) (6.0.2)\n",
      "Requirement already satisfied: numpy==1.17.3 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 45)) (1.17.3)\n",
      "Requirement already satisfied: olefile==0.46 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 46)) (0.46)\n",
      "Requirement already satisfied: pandocfilters==1.4.2 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 47)) (1.4.2)\n",
      "Requirement already satisfied: parso==0.5.1 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 48)) (0.5.1)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 49)) (0.7.5)\n",
      "Requirement already satisfied: Pillow==6.2.1 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 50)) (6.2.1)\n",
      "Requirement already satisfied: plac==1.1.3 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 51)) (1.1.3)\n",
      "Requirement already satisfied: preshed==3.0.2 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 52)) (3.0.2)\n",
      "Requirement already satisfied: prometheus-client==0.7.1 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 53)) (0.7.1)\n",
      "Requirement already satisfied: prompt-toolkit==2.0.10 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 54)) (2.0.10)\n",
      "Requirement already satisfied: pycparser==2.19 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 55)) (2.19)\n",
      "Requirement already satisfied: Pygments==2.4.2 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 56)) (2.4.2)\n",
      "Requirement already satisfied: pyrsistent==0.15.5 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 57)) (0.15.5)\n",
      "Requirement already satisfied: python-dateutil==2.8.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 58)) (2.8.0)\n",
      "Requirement already satisfied: pywin32==226 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 59)) (226)\n",
      "Requirement already satisfied: pywinpty==0.5.5 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 60)) (0.5.5)\n",
      "Requirement already satisfied: pyzmq==18.1.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 61)) (18.1.0)\n",
      "Requirement already satisfied: qtconsole==4.5.5 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 62)) (4.5.5)\n",
      "Requirement already satisfied: regex==2019.11.1 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 63)) (2019.11.1)\n",
      "Requirement already satisfied: requests==2.22.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 64)) (2.22.0)\n",
      "Requirement already satisfied: s3transfer==0.2.1 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 65)) (0.2.1)\n",
      "Requirement already satisfied: sacremoses==0.0.35 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 66)) (0.0.35)\n",
      "Requirement already satisfied: scikit-learn==0.21.3 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 67)) (0.21.3)\n",
      "Requirement already satisfied: scipy==1.3.2 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 68)) (1.3.2)\n",
      "Requirement already satisfied: Send2Trash==1.5.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 69)) (1.5.0)\n",
      "Requirement already satisfied: sentencepiece==0.1.83 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 70)) (0.1.83)\n",
      "Requirement already satisfied: six==1.12.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 71)) (1.12.0)\n",
      "Requirement already satisfied: spacy==2.2.2 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 72)) (2.2.2)\n",
      "Requirement already satisfied: srsly==0.2.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 73)) (0.2.0)\n",
      "Requirement already satisfied: terminado==0.8.2 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 74)) (0.8.2)\n",
      "Requirement already satisfied: testpath==0.4.4 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 75)) (0.4.4)\n",
      "Requirement already satisfied: thinc==7.3.1 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 76)) (7.3.1)\n",
      "Requirement already satisfied: torch==1.3.1 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 77)) (1.3.1)\n",
      "Requirement already satisfied: torchtext==0.4.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 78)) (0.4.0)\n",
      "Requirement already satisfied: torchvision==0.4.2 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 79)) (0.4.2)\n",
      "Requirement already satisfied: tornado==6.0.3 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 80)) (6.0.3)\n",
      "Requirement already satisfied: tqdm==4.38.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 81)) (4.38.0)\n",
      "Requirement already satisfied: traitlets==4.3.3 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 82)) (4.3.3)\n",
      "Requirement already satisfied: transformers==2.1.1 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 83)) (2.1.1)\n",
      "Requirement already satisfied: urllib3==1.25.6 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 84)) (1.25.6)\n",
      "Requirement already satisfied: wasabi==0.4.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 85)) (0.4.0)\n",
      "Requirement already satisfied: wcwidth==0.1.7 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 86)) (0.1.7)\n",
      "Requirement already satisfied: webencodings==0.5.1 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 87)) (0.5.1)\n",
      "Requirement already satisfied: widgetsnbextension==3.5.1 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 88)) (3.5.1)\n",
      "Requirement already satisfied: wincertstore==0.2 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 89)) (0.2)\n",
      "Requirement already satisfied: zipp==0.6.0 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from -r requirements.txt (line 90)) (0.6.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\nikmand\\appdata\\local\\continuum\\anaconda3\\envs\\ncsr-chatbot-py36\\lib\\site-packages (from ipython==7.9.0->-r requirements.txt (line 20)) (41.6.0.post20191030)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "To address this task we are going to need a pretrained language model.\n",
    "Many such state of the art models are included in the widely used library **_trasformers_** developped by **_Hugging Face_**. We choose to use GPT as it is pretrained on predicting the next word and compared to other models it's relatively small since our resources are limited. \n",
    "\n",
    "More details about the approach are provided in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "import copy\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from datetime import datetime\n",
    "from transformers import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer, WEIGHTS_NAME, CONFIG_NAME\n",
    "from itertools import chain\n",
    "from ast import literal_eval\n",
    "from itertools import zip_longest\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Model\n",
    "\n",
    "Below we instantiate a GPT Pytorch model with pre-trained weights on language modelling task along with tokenizer.\n",
    "Tokenizer is a helper class used to interact with the vocabulary in which our model has been pretrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(output_dir='openai-gpt'):\n",
    "    \"\"\"\n",
    "    Loads GPT Model and the corresponding tokenizer from local checkpoint.\n",
    "    If no path is specified the pretrained weights on language modelling task are downloaded.\n",
    "    \n",
    "    :param output_dir: path to checkpoint \n",
    "    :return : model and tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = OpenAIGPTTokenizer.from_pretrained(output_dir)\n",
    "    model = OpenAIGPTLMHeadModel.from_pretrained(output_dir)  \n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_checkpoint() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our language model have been pre-trained with a vocabulary of 40478 words.\n"
     ]
    }
   ],
   "source": [
    "print(\"Our language model have been pre-trained with a vocabulary of {} words.\".format(tokenizer.vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable model parameters: 116534784\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Number of trainable model parameters: {}\".format(trainable_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see GPT model has been loaded and it consists of 12 layers with a total of more than 116 millios of trainable parameters. We avoided using the more recent GPT-2 as it has 1.5 billions parameters and that may causes issues loading it into RAM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens_(model, tokenizer, special_tokens):\n",
    "    \"\"\"\n",
    "    Adds special tokens to tokenizer and model.\n",
    "    \"\"\"\n",
    "    orig_num_tokens = len(tokenizer.encoder)\n",
    "    num_added_tokens = tokenizer.add_special_tokens(special_tokens) # adds only if not present\n",
    "    \n",
    "    if num_added_tokens > 0:\n",
    "        print(\"New tokens added to model: {}\".format(num_added_tokens))\n",
    "        model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens) \n",
    "    else:\n",
    "        print(\"No new tokens found! Nothing added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw earlier that the pretrained model uses a vocabulary of around forty thousands words. Apart from those for our task we will need some extra tokens with special meaning. Hopefully tokenizer can handle this process for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New tokens added to model: 5\n"
     ]
    }
   ],
   "source": [
    "# various constants needed\n",
    "\n",
    "SPECIAL_TOKENS_DICT = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
    "                         'additional_special_tokens': ('<speaker1>', '<speaker2>')}\n",
    "\n",
    "max_history = 2 # pairs of question/answer to be retained\n",
    "max_sentence_length = 20 # maximum length of a sentence produced by the model \n",
    "\n",
    "temperature = 0.75 # increases confidence in the most propable outputs \n",
    "\n",
    "use_cuda = True # whether to try to use cuda or not\n",
    "\n",
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
    "add_special_tokens_(model, tokenizer, SPECIAL_TOKENS_DICT)  \n",
    "\n",
    "SPECIAL_TOKENS_IDS = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
    "TIME_FORMAT = '%Y%m%d_%H%M'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added 5 special tokens for denoting the start and the end of the sentences, the type of the input provided to the model(whether it came from the user or the bot) and a finally a token to denote padding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "In this section we define a series of functions to appropiate handle the data. At first with function **_parser_** we read the txt files and we keep only the dialogs in a list. Also we tokenize phrases and we convert all words to their corresponding ids in the model's vocabulary. This step is performed early before we create multiple copies of each sentence in many samples as part of history, to optimize performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(datafolder='metalwoz-v1/dialogues/', cache_file='cache_folder/dialogs.txt'): \n",
    "    \"\"\"\n",
    "    Function that reads files, keeps only 'turns' from each entry and tokenizes them\n",
    "\n",
    "    :param datafolder: path to the folder that contains the files\n",
    "    :param cache_file: filepath to save the result\n",
    "    :return: a list that contains dialogs, each dialog is a list of lists \n",
    "             where each of them represents the ids of a phrase,\n",
    "    \"\"\"\n",
    "    try:    # try to open cache file\n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            print(\"Cache file found loading content.\")\n",
    "            dialogs = pickle.load(f)\n",
    "            return dialogs\n",
    "        \n",
    "    except: # cache file not created yet\n",
    "        print(\"Cache file not found. Start processing.\")       \n",
    "    \n",
    "        dialogs = []\n",
    "        files = list(glob.glob(os.path.join(datafolder ,\"*.txt\")))\n",
    "\n",
    "        for file in files:\n",
    "            with open(file) as f:\n",
    "                for line in f.readlines():\n",
    "\n",
    "                    dialog = literal_eval(line)['turns'][1:] # keep only turns without the first sentence\n",
    "                    dialog = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(phrase)) for phrase in dialog]  \n",
    "                    dialogs.append(dialog) \n",
    "\n",
    "        if len(dialogs) > 0:\n",
    "            print(\"Saving parsed dialogs to file: {}\".format(cache_file))\n",
    "            with open(cache_file, \"wb\") as f: # save result so future calls can retrieve it right away\n",
    "                pickle.dump(dialogs, f)            \n",
    "                \n",
    "    return dialogs      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then with the function **_extract_pairs_** we use those dialogs to create utterances of input, output pairs. In every such pair we preserve in the input the entire past history of the dialog. Additionally we discard the first sentence , which is fixed, and the last one when it said by the user, as no bot answer follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pairs(dialogs = None, cache_file='cache_folder/pairs.txt'):\n",
    "    \"\"\"\n",
    "    Function that creates pairs of input, output from dialogs, each dialogs corresponds now to many pairs.\n",
    "    \n",
    "    :param dialogs: a list with all the dialogs \n",
    "    :param cache_file: filepath to save the result\n",
    "    \n",
    "    :return : a list whose elements are pairs of input(history), output(expected bot reply)  \n",
    "    \"\"\"\n",
    "    try:    # try to open cache file\n",
    "        \n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            print(\"Cache file found loading content.\")\n",
    "            pairs = pickle.load(f)       \n",
    "            return pairs\n",
    "        \n",
    "    except:      # cache file not created yet\n",
    "        print(\"Cache file not found. Start processing.\")\n",
    "        \n",
    "        pairs = [] \n",
    "        for dialog in dialogs:\n",
    "            \n",
    "            t_dict = {'input': []}\n",
    "            if len(dialog) % 2 != 0: # discard the last phrase if it was said by the user\n",
    "                dialog = dialog[:-1]    \n",
    "            dialog_it = iter(dialog)\n",
    "            \n",
    "            for i_phrase, o_phrase in zip_longest(dialog_it, dialog_it): # process phrases two by two        \n",
    "                try:\n",
    "                    t_dict[\"input\"].append(t_dict[\"output\"])\n",
    "                except:\n",
    "                    pass \n",
    "                t_dict[\"input\"].append(i_phrase) # history\n",
    "                t_dict[\"output\"] = o_phrase\n",
    "                pairs.append(t_dict)\n",
    "                t_dict = copy.deepcopy(t_dict) # so future changes address only the new dict\n",
    "           \n",
    "        if len(pairs) > 0:\n",
    "            print(\"Saving extracted pairs to file: {}\".format(cache_file))          \n",
    "            with open(cache_file, \"wb\") as f:  # save result so future calls can retrieve it right away\n",
    "                pickle.dump(pairs, f)\n",
    "            \n",
    "        return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of auxilary functions follows. At first, **_adjust_history_** helps us to tune the number of quenstion/answer pairs retained in history. Obviously in every case at least one sentence from the user is kept as input. We conducted training experiments for different values of retained history in order to observe if and it what extend it affects model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_history(pairs, max_history=2): \n",
    "    \"\"\"\n",
    "    Reduces number of previous chat senteces that are going to be included in the input\n",
    "    \n",
    "    :param pairs: list with samples \n",
    "    :param max_history: Number of pairs of question/answer to be preserved (at least one is preserved)\n",
    "    :return : two lists, pairs with fixed history and theirs corresponding seq_lenghts \n",
    "    \"\"\"\n",
    "    \n",
    "    pairs_len = []\n",
    "    for pair in pairs:\n",
    "        \n",
    "        pair['input'] = pair['input'][-(2*max_history+1):] # at least one phrase is preserved\n",
    "        pair_len = sum(len(phrase) for phrase in pair['input']) + len(pair['output'])\n",
    "        pairs_len.append(pair_len)\n",
    "        \n",
    "    return pairs, pairs_len   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, **_filter_samples_** assesses the samples based on their sequense length. Samples that their length exceeds a specified percentile are dropped. The impact of this function is crusial in terms of performance as we will see later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_samples(samples, samples_len, percentile=90):\n",
    "    \"\"\"\n",
    "    Filters samples based on sequence lengths.  \n",
    "    \n",
    "    :param samples: a list with samples\n",
    "    :param samples_len: their corresponding lengths\n",
    "    :param percentile: percentage of samples to preserve  \n",
    "    \n",
    "    :return : two lists, preserved samples and their lengths\n",
    "    \"\"\"\n",
    "    \n",
    "    samples_length = np.array(samples_len)\n",
    "    reasonable_length = np.percentile(samples_length, percentile)\n",
    "    print(\"{}% of the samples have sequence length less than {}\".format(percentile, reasonable_length))\n",
    "    \n",
    "    samples_red, samples_len_red = [], []\n",
    "    for sample, sample_len in zip(samples, samples_len):\n",
    "        \n",
    "        if sample_len <= reasonable_length:\n",
    "            samples_red.append(sample)\n",
    "            samples_len_red.append(sample_len)\n",
    "    \n",
    "    return samples_red, samples_len_red  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing all the aforementioned procedures leads to the following results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache file found loading content.\n",
      "Number of dialogs in the whole dataset: 37884\n",
      "Cache file found loading content.\n",
      "Number of pairs created: 193985\n",
      "Maximum seq_length observed: 684\n",
      "90% of the samples have sequence length less than 79.0\n",
      "Number of pairs retained: 175289\n"
     ]
    }
   ],
   "source": [
    "dialogs = parser()\n",
    "\n",
    "print(\"Number of dialogs in the whole dataset: {}\".format(len(dialogs)))\n",
    "\n",
    "pairs = extract_pairs(dialogs) # list of dictionaries of input history and bot's reply\n",
    "\n",
    "# keep only portion of the chat history to reduce seq_length\n",
    "pairs, pairs_len = adjust_history(pairs, max_history=max_history) \n",
    "\n",
    "print(\"Number of pairs created: {}\".format(len(pairs)))\n",
    "print(\"Maximum seq_length observed: {}\".format(max(pairs_len)))\n",
    "\n",
    "pairs_reduced, pairs_len_reduced = filter_samples(pairs, pairs_len) \n",
    "print(\"Number of pairs retained: {}\".format(len(pairs_reduced)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that almost 200000 samples were created with a max sequence length of 684 tokens. This is a huge problems for training as every batch that comes into the model is a tensor with fixed dimensions. So the shape of this tensor is decided by the sample with the maximum length on the batch(smaller samples are padded). As a result training times increase dramatically.\n",
    "\n",
    "Using **_filter_samples_** to cut off the largest 10% of the samples reduces the sequnce length to 79, which something that we can handle in compination with other techniques used afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shaping Data for training\n",
    "\n",
    "For the needs of training procedure we split our dataset to train and validation set. Validation set is used to monitor the performance of the model at every training epoch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train set: 122702 and in validation: 52587\n"
     ]
    }
   ],
   "source": [
    "pairs_train, pairs_eval = train_test_split(pairs_reduced, test_size=0.3, shuffle=True)      \n",
    "\n",
    "print(\"Number of samples in train set: {} and in validation: {}\".format(len(pairs_train), len(pairs_eval)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to our model consists of 4 components. Firstly the history along with the so far produced reply are given to the model. Apart from that the type(whether each token came from the user or the bot)of the input is provided. The labels, namely the tokens of the reply are provided again separately to the model and CrossEntropyLoss is returned. Final segment is the attention mask which denotes where real values ends, to prevent attention mechanism for taking into consideration padding.\n",
    "\n",
    "Given the specified arguments the above procedure is calculated by the function **_create_model_inputs_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs(history, reply, tokenizer, with_eos=True):\n",
    "    \"\"\"\n",
    "    Function that creates the various parts of the model input from input/output pairs.\n",
    "    \"\"\"\n",
    "    \n",
    "    bos, eos, speaker1, speaker2 = SPECIAL_TOKENS_IDS[:-1]\n",
    "    sequence = [[bos]] + history + [reply + ([eos] if with_eos else [])]\n",
    "    seq_len = len(sequence) # sequence: list of lists\n",
    "    sequence = [sequence[0]] + [[speaker2 if (seq_len-i) % 2 != 1 else speaker1] + s \n",
    "                                for i, s in enumerate(sequence[1:])]\n",
    "    \n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence)) # words\n",
    "    instance[\"token_type_ids\"] = [speaker1] + [speaker2 if i % 2 else speaker1 \n",
    "                                               for i, s in enumerate(sequence[1:]) for _ in s] # for each word\n",
    "    instance[\"mask\"] = [1] * len(instance[\"input_ids\"])                                     # attention mask\n",
    "    instance[\"lm_labels\"] = ([-1] * sum(len(s) for s in sequence[:-1])) + [-1] + sequence[-1][1:]\n",
    "    # TODO positional embeddings\n",
    "    \n",
    "    return instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next blocks we use Dataset and Dataloader classes to help us create batches from our data. DialogDataset class wraps the data and it takes care of creating input segments from samples as well as it sorts samples to optimize padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that wraps data. It takes care of creating input segments from samples as well as\n",
    "    it sorts samples to optimize padding. Used by the dataloader to sample entries.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dialog_pairs):\n",
    "        self.dataset = self.create_segments(dialog_pairs) # create the segments of the input\n",
    "        self.dataset = self.sort_on_seq_length() # sorting\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def create_segments(self, dialog_pairs):\n",
    "        \"\"\"\n",
    "        Creates input segments for each sample\n",
    "        \"\"\"\n",
    "        dataset = []\n",
    "        for pair in dialog_pairs:\n",
    "            instance = create_model_inputs(pair['input'], pair['output'], tokenizer)\n",
    "            dataset.append(instance)\n",
    "        return dataset\n",
    "    \n",
    "    def sort_on_seq_length(self): # could be optimized to use bucket sorting as the number of sample is big\n",
    "        \"\"\"\n",
    "        Sorts dataset based on seq_len to minimize padding afterwards\n",
    "        \"\"\"\n",
    "        return sorted(self.dataset, key=lambda x: len(x['input_ids']))\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return  self.dataset[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader has the role of getting samples from the specified Dataset object and collate them together to form batches. We want to avoid padding the dataset in the global level as by doing so every batch will be of the maximum sequence length. Instead we sorted all samples insided dataset class based on their length and we are preventing Dataloader to sample them at random so for this order to be preserved. \n",
    "\n",
    "In this way samples of similar size end up in the same batch and padding is performed inside **_custom_collate_fn_** on a batch level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequenses(batch, pad_token=0):\n",
    "    \"\"\"\n",
    "    Pads a list of tokens. Padding token differentiates for each input segment \n",
    "    \"\"\"\n",
    "    max_seq_len = max(len(entry[\"input_ids\"]) for entry in batch)\n",
    "    \n",
    "    for entry in batch:\n",
    "        for index_name in entry.keys():\n",
    "            \n",
    "            if index_name == \"lm_labels\":\n",
    "                pad_token_ = -1\n",
    "            elif index_name == \"mask\":\n",
    "                pad_token_ = 0\n",
    "            else:\n",
    "                pad_token_ = pad_token\n",
    "                \n",
    "            entry[index_name] =  entry[index_name] + [pad_token_] * (max_seq_len - len(entry[index_name]))\n",
    "  \n",
    "    return batch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Function that is provided by samples and stacks them together to form tensors.\n",
    "    We call padding function here. \n",
    "    \n",
    "    :param batch: a list of sambles\n",
    "    :reutrn : input segments as tensors\n",
    "    \"\"\"\n",
    "    batch = pad_sequenses(batch, SPECIAL_TOKENS_IDS[-1])\n",
    "    \n",
    "    inputs = [torch.stack(list(map(lambda x: torch.from_numpy( \\\n",
    "        np.array(x[index_name])), batch)), dim=0) for index_name in batch[0].keys()]\n",
    "\n",
    "    inputs = [input_tensor.type(torch.LongTensor) for input_tensor in inputs]\n",
    "\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "        inputs = [input_tensor.cuda() for input_tensor in inputs]   \n",
    "\n",
    "    return inputs  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part is the creation of the DialogDataset objects and the call to the DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = DialogDataset(pairs_train) \n",
    "validation_set = DialogDataset(pairs_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 32 \n",
    "EVAL_BATCH_SIZE = 64 \n",
    "\n",
    "dataloader_train = DataLoader(training_set, batch_size=TRAIN_BATCH_SIZE, shuffle=False, \n",
    "                              collate_fn=custom_collate_fn, num_workers=0) \n",
    "\n",
    "dataloader_valid = DataLoader(validation_set, batch_size=EVAL_BATCH_SIZE, shuffle=False,\n",
    "                              collate_fn=custom_collate_fn, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of the batch sizes was made carefully as the model along with the batch of the biggest sequence length must be able to fit in RAM memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, print_period=400):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    epoch_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for i_batch, (input_ids, attention_mask, category_ids, label_ids) in enumerate(dataloader):\n",
    "        \n",
    "        loss, logits = model(input_ids, attention_mask, category_ids, labels=label_ids)\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()  \n",
    "        \n",
    "    return epoch_loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    epoch_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i_batch, (input_ids, attention_mask, category_ids, label_ids) in enumerate(dataloader):\n",
    "            \n",
    "            loss, logits = model(input_ids, attention_mask, category_ids, labels=label_ids)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint_state(model, tokenizer, output_dir=None):\n",
    "    \"\"\"\n",
    "    Function that saves models weights and configuration as well as tokenizer's voc.\n",
    "    \n",
    "    :param model: to checkpoint\n",
    "    :param tokenizer: to checkpoint\n",
    "    :param output_dir: directory where checkpointed files will be created\n",
    "    :retun :\n",
    "    \"\"\"\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = 'metalwoz-v1/checkpoint_{}'.format(datetime.now().strftime(TIME_FORMAT))\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(output_dir)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "    output_config_file = os.path.join(output_dir, CONFIG_NAME)  \n",
    "    \n",
    "    torch.save(model.state_dict(), output_model_file) # checkpoint weights\n",
    "    model.config.to_json_file(output_config_file)     # configuration\n",
    "    tokenizer.save_vocabulary(output_dir)             # vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingException(Exception):\n",
    "    def __init__(self, message):\n",
    "\n",
    "        super().__init__(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping(min_loss, cur_patience, max_patience, epoch_eval_loss):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if (epoch_eval_loss >= min_loss):  \n",
    "        cur_patience += 1\n",
    "        if (cur_patience >= max_patience):\n",
    "            raise EarlyStoppingException(\"Execution terminated due to Early Stopping\")\n",
    "    else:\n",
    "        print(\"New min validation loss}\")\n",
    "        checkpoint_state(model, tokenizer) # checkpointing\n",
    "        print(\"New checkpoint created\")\n",
    "        min_loss, cur_patience = epoch_eval_loss, 0    \n",
    "        \n",
    "    return min_loss, cur_patience    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_procedure(epochs, lr, use_cuda, min_loss, max_patience, cur_patience):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # , weight_decay=0.001 # TODO review those values\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_loss = train(model, dataloader_train, optimizer) / len(dataloader_train)\n",
    "        print(\"Loss on train set: \\t\\t epoch {} : {:.4f}\".format(epoch, train_loss))\n",
    "\n",
    "        eval_loss = evaluate(model, dataloader_valid) / len(dataloader_valid)     \n",
    "        print(\"Loss on validation set: \\t epoch {} : {:.4f}\".format(epoch, eval_loss))    \n",
    "\n",
    "        try:\n",
    "            min_loss, cur_patience = early_stopping(min_loss, cur_patience, max_patience, eval_loss) \n",
    "        except EarlyStoppingException as e:\n",
    "            print(\"{} at epoch: {}\".format(e, epoch))\n",
    "            break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on train set: \t\t epoch 0 : 6.6461\n",
      "Loss on validation set: \t epoch 0 : 6.3160\n",
      "New min validation loss}\n",
      "New checkpoint created\n",
      "Loss on train set: \t\t epoch 1 : 6.1977\n",
      "Loss on validation set: \t epoch 1 : 5.8961\n",
      "New min validation loss}\n",
      "New checkpoint created\n",
      "Loss on train set: \t\t epoch 2 : 5.9382\n",
      "Loss on validation set: \t epoch 2 : 5.8530\n",
      "New min validation loss}\n",
      "New checkpoint created\n",
      "Loss on train set: \t\t epoch 3 : 5.8087\n",
      "Loss on validation set: \t epoch 3 : 5.8559\n",
      "Loss on train set: \t\t epoch 4 : 5.7147\n",
      "Loss on validation set: \t epoch 4 : 5.8270\n",
      "New min validation loss}\n",
      "New checkpoint created\n",
      "Loss on train set: \t\t epoch 5 : 5.6415\n",
      "Loss on validation set: \t epoch 5 : 5.8311\n",
      "Loss on train set: \t\t epoch 6 : 5.5931\n",
      "Loss on validation set: \t epoch 6 : 5.8536\n",
      "Loss on train set: \t\t epoch 7 : 5.5158\n",
      "Loss on validation set: \t epoch 7 : 5.8668\n",
      "Execution terminated due to Early Stopping at epoch: 7\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 6.25e-5\n",
    "min_loss, max_patience, cur_patience = np.inf, 3, 0\n",
    "\n",
    "train_procedure(epochs, lr, use_cuda, min_loss, max_patience, cur_patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction with the bot - Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(history, reply_so_far):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    history = [tokenizer.encode(phrase) for phrase in history]\n",
    "    \n",
    "    instance = create_model_inputs(history, reply_so_far, tokenizer, with_eos=False)\n",
    "    \n",
    "    input_ids = torch.tensor(instance[\"input_ids\"]).unsqueeze(0)\n",
    "    token_type_ids = torch.tensor(instance[\"token_type_ids\"]).unsqueeze(0)\n",
    "    \n",
    "    return input_ids, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding(probs, logits, method=\"top_p\"):\n",
    "    \"\"\"\n",
    "    Functions that selects the next token to be emmited. Three different approaches are implemented: \n",
    "    \n",
    "    Greedy: the most probable token is selected.\n",
    "    Top-k : \n",
    "    Top-p : \n",
    "    \n",
    "    :param logits: \n",
    "    :param method: the decoding method to be used, Values={'greedy', 'top_k', 'top_p'}\n",
    "    :return: the selected token\n",
    "    \"\"\"\n",
    "    top_k = 40 # sample from the 100 most probable tokens based on their probs\n",
    "    top_p = 0.9 # sample from the n most probable tokens that have a cumulative probability at least 0.9 \n",
    "    \n",
    "    if method == \"greedy\":\n",
    "        return torch.argmax(probs).item()\n",
    "    \n",
    "    elif method == \"top_k\":        \n",
    "        prob_k = probs.topk(top_k)[0][-1].item() # value of the 100th most probable\n",
    "        probs[probs < prob_k] = 0   # cut off the tail  \n",
    "        \n",
    "    elif method == \"top_p\":\n",
    "        probs_sorted, probs_indexes = probs.sort(dim=-1, descending=True) # start the cumulation from the most probable token in descending order\n",
    "        cum_probs = probs_sorted.cumsum(dim=-1)\n",
    "        \n",
    "        indices = cum_probs > top_p \n",
    "        indices[1:] = indices[:-1].clone()\n",
    "        indices[0] = 0 # at least one token is preserved \n",
    "        \n",
    "        probs[probs_indexes[indices]] = 0\n",
    "    \n",
    "    word = torch.multinomial(probs, 1).item()\n",
    "    # TODO handle the case that special token was emitted in the first pick\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_answer(history, model, tokenizer, method=\"top_p\"):\n",
    "    \"\"\"\n",
    "    Function that generates word by word the bot answer, based on user input and previous history.\n",
    "    \n",
    "    :param history: a list of past sentences and last user's input, in plain text\n",
    "    :param model: the model to be used for inference\n",
    "    :return: a list with the words of the answer in plain text \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    reply_so_far = []\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i in range(max_sentence_length):\n",
    "            \n",
    "            input_ids, category_ids = format_input(history, reply_so_far)\n",
    "            outputs = model(input_ids=input_ids, token_type_ids=category_ids)\n",
    "            logits = outputs[0]\n",
    "            logits = logits[0, -1, :] / temperature # keep last \n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            word = decoding(probs, logits, method=method) \n",
    "            \n",
    "            if word in SPECIAL_TOKENS_IDS: # we stop inference if we find a special token without emitting this token\n",
    "                print(\"Bot terminate sentence!\")\n",
    "                break\n",
    "            reply_so_far.append(word)\n",
    "            \n",
    "        answer_text = tokenizer.decode(reply_so_far, skip_special_tokens=True)    \n",
    "        return answer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_with_bot(model, tokenizer, method='top_p'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    bot_prompt = \"bot:>>> \"\n",
    "    user_prompt = \"user:>>> \"\n",
    "\n",
    "    history = []\n",
    "    print(bot_prompt + \"Hello how may I help you?\")\n",
    "    user_input = input(user_prompt)\n",
    "    \n",
    "    while user_input != \"\\q\": # TODO check if we need to truncate user input to not exceed max_length\n",
    "        \n",
    "        history.append(user_input)\n",
    "        answer = infer_answer(history, model, tokenizer, method=method)\n",
    "        history.append(answer)\n",
    "        \n",
    "        history = history[-(2*max_history+1):]  # keep the same history as in the training \n",
    "        \n",
    "        print(bot_prompt + answer)\n",
    "        \n",
    "        user_input = input(user_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikmand\\nikmand\\ncsr-chatbot\\metalwoz-v1\\checkpoint_20191114_1250\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"metalwoz-v1/checkpoint_20191114_1250\"\n",
    "\n",
    "model_loaded ,tokenizer_loaded  = load_checkpoint(checkpoint_dir)\n",
    "\n",
    "add_special_tokens_(model_loaded, tokenizer_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bot:>>> Hello how may I help you?\n",
      "user:>>> i need toothpaste\n",
      "Bot terminate sentence!\n",
      "bot:>>> \n",
      "user:>>> hey how are you?\n",
      "Bot terminate sentence!\n",
      "bot:>>> yes\n",
      "user:>>> tell me something.\n",
      "Bot terminate sentence!\n",
      "bot:>>> yes\n",
      "user:>>> \\q\n"
     ]
    }
   ],
   "source": [
    "interact_with_bot(model_loaded, tokenizer_loaded, method='top_p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
