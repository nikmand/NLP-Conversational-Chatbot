{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: Create a Chatbot\n",
    "\n",
    "### Name: Nikiforos Mandilaras\n",
    "\n",
    "### Email: nikiforosmandi@windowslive.com\n",
    "\n",
    "### Date: 15/11/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This Notebook was developed as an assignment for the needs of National Center of Scientific Research Demokritos.\n",
    "The goal is to create a chatbot, namely a conversatonal model that can give responses to the user's inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Packaging & Execution\n",
    "\n",
    "In order to be able to execute this jupyter notebook, all depedent packages must be present. For this reason we save those along with their versions in requirements.txt (which is included in the submission folder). \n",
    "\n",
    "Then we can install them all at once using pip as it can be seen below. To avoid any collisions with existing packages it is necessary to create a new python virtual enviroment and run the command inside there. \n",
    "\n",
    "To access data the following code uses relative paths, assuming data are present in the same folder as the jupyter notebook. However all functions can accept absolute paths as well.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to install dependencies\n",
    "\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "\n",
    "To address this task we are going to need a pretrained language model.\n",
    "Many such state of the art models are included in the widely used library [trasformers](https://github.com/huggingface/transformers) [1] developped by **_Hugging Face_**. We choose to use GPT as it is pretrained on predicting the next word and compared to other models it's relatively small since our resources are limited. \n",
    "\n",
    "More details about the approach are provided in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "import copy\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from datetime import datetime\n",
    "from transformers import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer, WEIGHTS_NAME, CONFIG_NAME\n",
    "from itertools import chain\n",
    "from ast import literal_eval\n",
    "from itertools import zip_longest\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Model\n",
    "\n",
    "Below we instantiate a GPT Pytorch model with pre-trained weights on language modelling task along with tokenizer.\n",
    "Tokenizer is a helper class used to interact with the vocabulary in which our model has been pretrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(output_dir='openai-gpt'):\n",
    "    \"\"\"\n",
    "    Loads GPT Model and the corresponding tokenizer from local checkpoint.\n",
    "    If no path is specified the pretrained weights on language modelling task are downloaded.\n",
    "    \n",
    "    :param output_dir: path to checkpoint \n",
    "    :return : model and tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = OpenAIGPTTokenizer.from_pretrained(output_dir)\n",
    "    model = OpenAIGPTLMHeadModel.from_pretrained(output_dir)  \n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_checkpoint() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our language model have been pre-trained with a vocabulary of 40478 words.\n"
     ]
    }
   ],
   "source": [
    "print(\"Our language model have been pre-trained with a vocabulary of {} words.\".format(tokenizer.vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable model parameters: 116534784\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Number of trainable model parameters: {}\".format(trainable_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see GPT model has been loaded and it consists of 12 layers with a total of more than 116 millios of trainable parameters. We avoided using the more recent GPT-2 as it has 1.5 billions parameters and that may causes issues loading it into RAM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens_(model, tokenizer, special_tokens):\n",
    "    \"\"\"\n",
    "    Adds special tokens to tokenizer and model.\n",
    "    \"\"\"\n",
    "    orig_num_tokens = len(tokenizer.encoder)\n",
    "    num_added_tokens = tokenizer.add_special_tokens(special_tokens) # adds only if not present\n",
    "    \n",
    "    if num_added_tokens > 0:\n",
    "        print(\"New tokens added to model: {}\".format(num_added_tokens))\n",
    "        model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_added_tokens) \n",
    "    else:\n",
    "        print(\"No new tokens found! Nothing added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw earlier that the pretrained model uses a vocabulary of around forty thousands words. Apart from those for our task we will need some extra tokens with special meaning. Hopefully tokenizer can handle this process for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# various constants needed\n",
    "\n",
    "SPECIAL_TOKENS_DICT = {'bos_token': '<bos>', 'eos_token': '<eos>', 'pad_token': '<pad>',\n",
    "                         'additional_special_tokens': ('<speaker1>', '<speaker2>')}\n",
    "\n",
    "max_history = 2 # pairs of question/answer to be retained\n",
    "max_sentence_length = 20 # maximum length of a sentence produced by the model \n",
    "\n",
    "temperature = 0.75 # increases confidence in the most propable outputs \n",
    "\n",
    "use_cuda = True # whether to try to use cuda or not\n",
    "\n",
    "SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
    "add_special_tokens_(model, tokenizer, SPECIAL_TOKENS_DICT)  \n",
    "\n",
    "SPECIAL_TOKENS_IDS = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
    "TIME_FORMAT = '%Y%m%d_%H%M'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added 5 special tokens for denoting the start and the end of the sentences, the type of the input provided to the model(whether it came from the user or the bot) and a finally a token to denote padding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "In this section we define a series of functions to appropiate handle the data. At first with function **_parser_** we read the txt files and we keep only the dialogs in a list. Also we tokenize phrases and we convert all words to their corresponding ids in the model's vocabulary. This step is performed early before we create multiple copies of each sentence in many samples as part of history, to optimize performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(datafolder='metalwoz-v1/dialogues/', cache_file='cache_folder/dialogs.txt'): \n",
    "    \"\"\"\n",
    "    Function that reads files, keeps only 'turns' from each entry and tokenizes them\n",
    "\n",
    "    :param datafolder: path to the folder that contains the files\n",
    "    :param cache_file: filepath to save the result\n",
    "    :return: a list that contains dialogs, each dialog is a list of lists \n",
    "             where each of them represents the ids of a phrase,\n",
    "    \"\"\"\n",
    "    try:    # try to open cache file\n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            print(\"Cache file found loading content.\")\n",
    "            dialogs = pickle.load(f)\n",
    "            return dialogs\n",
    "        \n",
    "    except: # cache file not created yet\n",
    "        print(\"Cache file not found. Start processing.\")       \n",
    "    \n",
    "        dialogs = []\n",
    "        files = list(glob.glob(os.path.join(datafolder ,\"*.txt\")))\n",
    "\n",
    "        for file in files:\n",
    "            with open(file) as f:\n",
    "                for line in f.readlines():\n",
    "\n",
    "                    dialog = literal_eval(line)['turns'][1:] # keep only turns without the first sentence\n",
    "                    dialog = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(phrase)) for phrase in dialog]  \n",
    "                    dialogs.append(dialog) \n",
    "\n",
    "        if len(dialogs) > 0:\n",
    "            print(\"Saving parsed dialogs to file: {}\".format(cache_file))\n",
    "            with open(cache_file, \"wb\") as f: # save result so future calls can retrieve it right away\n",
    "                pickle.dump(dialogs, f)            \n",
    "                \n",
    "    return dialogs      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then with the function **_extract_pairs_** we use those dialogs to create utterances of input, output pairs. In every such pair we preserve in the input the entire past history of the dialog. Additionally we discard the first sentence , which is fixed, and the last one when it said by the user, as no bot answer follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pairs(dialogs = None, cache_file='cache_folder/pairs.txt'):\n",
    "    \"\"\"\n",
    "    Function that creates pairs of input, output from dialogs, each dialogs corresponds now to many pairs.\n",
    "    \n",
    "    :param dialogs: a list with all the dialogs \n",
    "    :param cache_file: filepath to save the result\n",
    "    \n",
    "    :return : a list whose elements are pairs of input(history), output(expected bot reply)  \n",
    "    \"\"\"\n",
    "    try:    # try to open cache file\n",
    "        \n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            print(\"Cache file found loading content.\")\n",
    "            pairs = pickle.load(f)       \n",
    "            return pairs\n",
    "        \n",
    "    except:      # cache file not created yet\n",
    "        print(\"Cache file not found. Start processing.\")\n",
    "        \n",
    "        pairs = [] \n",
    "        for dialog in dialogs:\n",
    "            \n",
    "            t_dict = {'input': []}\n",
    "            if len(dialog) % 2 != 0: # discard the last phrase if it was said by the user\n",
    "                dialog = dialog[:-1]    \n",
    "            dialog_it = iter(dialog)\n",
    "            \n",
    "            for i_phrase, o_phrase in zip_longest(dialog_it, dialog_it): # process phrases two by two        \n",
    "                try:\n",
    "                    t_dict[\"input\"].append(t_dict[\"output\"])\n",
    "                except:\n",
    "                    pass \n",
    "                t_dict[\"input\"].append(i_phrase) # history\n",
    "                t_dict[\"output\"] = o_phrase\n",
    "                pairs.append(t_dict)\n",
    "                t_dict = copy.deepcopy(t_dict) # so future changes address only the new dict\n",
    "           \n",
    "        if len(pairs) > 0:\n",
    "            print(\"Saving extracted pairs to file: {}\".format(cache_file))          \n",
    "            with open(cache_file, \"wb\") as f:  # save result so future calls can retrieve it right away\n",
    "                pickle.dump(pairs, f)\n",
    "            \n",
    "        return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of auxilary functions follows. At first, **_adjust_history_** helps us to tune the number of quenstion/answer pairs retained in history. Obviously in every case at least one sentence from the user is kept as input. We conducted training experiments for different values of retained history in order to observe if and it what extend it affects model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_history(pairs, max_history=2): \n",
    "    \"\"\"\n",
    "    Reduces number of previous chat senteces that are going to be included in the input\n",
    "    \n",
    "    :param pairs: list with samples \n",
    "    :param max_history: Number of pairs of question/answer to be preserved (at least one is preserved)\n",
    "    :return : two lists, pairs with fixed history and theirs corresponding seq_lenghts \n",
    "    \"\"\"\n",
    "    \n",
    "    pairs_len = []\n",
    "    for pair in pairs:\n",
    "        \n",
    "        pair['input'] = pair['input'][-(2*max_history+1):] # at least one phrase is preserved\n",
    "        pair_len = sum(len(phrase) for phrase in pair['input']) + len(pair['output'])\n",
    "        pairs_len.append(pair_len)\n",
    "        \n",
    "    return pairs, pairs_len   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, **_filter_samples_** assesses the samples based on their sequense length. Samples that their length exceeds a specified percentile are dropped. The impact of this function is crusial in terms of performance as we will see later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_samples(samples, samples_len, percentile=90):\n",
    "    \"\"\"\n",
    "    Filters samples based on sequence lengths.  \n",
    "    \n",
    "    :param samples: a list with samples\n",
    "    :param samples_len: their corresponding lengths\n",
    "    :param percentile: percentage of samples to preserve  \n",
    "    \n",
    "    :return : two lists, preserved samples and their lengths\n",
    "    \"\"\"\n",
    "    \n",
    "    samples_length = np.array(samples_len)\n",
    "    reasonable_length = np.percentile(samples_length, percentile)\n",
    "    print(\"{}% of the samples have sequence length less than {}\".format(percentile, reasonable_length))\n",
    "    \n",
    "    samples_red, samples_len_red = [], []\n",
    "    for sample, sample_len in zip(samples, samples_len):\n",
    "        \n",
    "        if sample_len <= reasonable_length:\n",
    "            samples_red.append(sample)\n",
    "            samples_len_red.append(sample_len)\n",
    "    \n",
    "    return samples_red, samples_len_red  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing all the aforementioned procedures leads to the following results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache file found loading content.\n",
      "Number of dialogs in the whole dataset: 37884\n",
      "Cache file found loading content.\n",
      "Number of pairs created: 193985\n",
      "Maximum seq_length observed: 684\n",
      "90% of the samples have sequence length less than 79.0\n",
      "Number of pairs retained: 175289\n"
     ]
    }
   ],
   "source": [
    "dialogs = parser()\n",
    "\n",
    "print(\"Number of dialogs in the whole dataset: {}\".format(len(dialogs)))\n",
    "\n",
    "pairs = extract_pairs(dialogs) # list of dictionaries of input history and bot's reply\n",
    "\n",
    "# keep only portion of the chat history to reduce seq_length\n",
    "pairs, pairs_len = adjust_history(pairs, max_history=max_history) \n",
    "\n",
    "print(\"Number of pairs created: {}\".format(len(pairs)))\n",
    "print(\"Maximum seq_length observed: {}\".format(max(pairs_len)))\n",
    "\n",
    "pairs_reduced, pairs_len_reduced = filter_samples(pairs, pairs_len) \n",
    "print(\"Number of pairs retained: {}\".format(len(pairs_reduced)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that almost 200000 samples were created with a max sequence length of 684 tokens. This is a huge problems for training as every batch that comes into the model is a tensor with fixed dimensions. So the shape of this tensor is decided by the sample with the maximum length on the batch(smaller samples are padded). As a result training times increase dramatically.\n",
    "\n",
    "Using **_filter_samples_** to cut off the largest 10% of the samples reduces the sequnce length to 79, which something that we can handle in compination with other techniques used afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shaping Data for training\n",
    "\n",
    "For the needs of training procedure we split our dataset to train and validation set. Validation set is used to monitor the performance of the model at every training epoch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train set: 122702 and in validation: 52587\n"
     ]
    }
   ],
   "source": [
    "pairs_train, pairs_eval = train_test_split(pairs_reduced, test_size=0.3, shuffle=True)      \n",
    "\n",
    "print(\"Number of samples in train set: {} and in validation: {}\".format(len(pairs_train), len(pairs_eval)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to our model consists of 4 components. Firstly the history along with the expected reply are given to the model. Apart from that the type(whether each token came from the user or the bot) is provided. \n",
    "\n",
    "The labels, namely the tokens of the reply are provided again separately to the model. The labels are shifted inside the model by one position so that outputs that are in position before the n token are trained to predict the token at position n. The output of the model is a 3D tensor with shape (batch_size, sequence_length, vocabulary_size). **_CrossEntropyLoss_** is calculated between this tensor and provided labels. \n",
    "\n",
    "Final segment is the attention mask which denotes where real values ends, to prevent attention mechanism for taking into consideration padding. \n",
    "\n",
    "Insiration about the inputs of the model was taken by [this](https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313) medium tutorial [2]. \n",
    "\n",
    "Given the specified arguments the above procedure is calculated by the function **_create_model_inputs_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs(history, reply, tokenizer, with_eos=True):\n",
    "    \"\"\"\n",
    "    Function that creates the various parts of the model input from input/output pairs.\n",
    "    \"\"\"\n",
    "    \n",
    "    bos, eos, speaker1, speaker2 = SPECIAL_TOKENS_IDS[:-1]\n",
    "    sequence = [[bos]] + history + [reply + ([eos] if with_eos else [])]\n",
    "    seq_len = len(sequence) # sequence: list of lists\n",
    "    sequence = [sequence[0]] + [[speaker2 if (seq_len-i) % 2 != 1 else speaker1] + s \n",
    "                                for i, s in enumerate(sequence[1:])]\n",
    "    \n",
    "    instance = {}\n",
    "    instance[\"input_ids\"] = list(chain(*sequence)) # words\n",
    "    instance[\"token_type_ids\"] = [speaker1] + [speaker2 if i % 2 else speaker1 \n",
    "                                               for i, s in enumerate(sequence[1:]) for _ in s] # for each word\n",
    "    instance[\"mask\"] = [1] * len(instance[\"input_ids\"])                                     # attention mask\n",
    "    instance[\"lm_labels\"] = ([-1] * sum(len(s) for s in sequence[:-1])) + [-1] + sequence[-1][1:]\n",
    "    # TODO positional embeddings\n",
    "    \n",
    "    return instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next blocks we use Dataset and Dataloader classes to help us create batches from our data. DialogDataset class wraps the data and it takes care of creating input segments from samples as well as it sorts samples to optimize padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that wraps data. It takes care of creating input segments from samples as well as\n",
    "    it sorts samples to optimize padding. Used by the dataloader to sample entries.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dialog_pairs):\n",
    "        self.dataset = self.create_segments(dialog_pairs) # create the segments of the input\n",
    "        self.dataset = self.sort_on_seq_length() # sorting\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def create_segments(self, dialog_pairs):\n",
    "        \"\"\"\n",
    "        Creates input segments for each sample\n",
    "        \"\"\"\n",
    "        dataset = []\n",
    "        for pair in dialog_pairs:\n",
    "            instance = create_model_inputs(pair['input'], pair['output'], tokenizer)\n",
    "            dataset.append(instance)\n",
    "        return dataset\n",
    "    \n",
    "    def sort_on_seq_length(self): # could be optimized to use bucket sorting as the number of sample is big\n",
    "        \"\"\"\n",
    "        Sorts dataset based on seq_len to minimize padding afterwards\n",
    "        \"\"\"\n",
    "        return sorted(self.dataset, key=lambda x: len(x['input_ids']))\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return  self.dataset[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader has the role of getting samples from the specified Dataset object and collate them together to form batches. We want to avoid padding the dataset in the global level as by doing so every batch will be of the maximum sequence length. Instead we sorted all samples insided dataset class based on their length and we are preventing Dataloader to sample them at random so for this order to be preserved. \n",
    "\n",
    "In this way samples of similar size end up in the same batch and padding is performed inside **_custom_collate_fn_** on a batch level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequenses(batch, pad_token=0):\n",
    "    \"\"\"\n",
    "    Pads a list of tokens. Padding token differentiates for each input segment \n",
    "    \"\"\"\n",
    "    max_seq_len = max(len(entry[\"input_ids\"]) for entry in batch)\n",
    "    \n",
    "    for entry in batch:\n",
    "        for index_name in entry.keys():\n",
    "            \n",
    "            if index_name == \"lm_labels\":\n",
    "                pad_token_ = -1\n",
    "            elif index_name == \"mask\":\n",
    "                pad_token_ = 0\n",
    "            else:\n",
    "                pad_token_ = pad_token\n",
    "                \n",
    "            entry[index_name] =  entry[index_name] + [pad_token_] * (max_seq_len - len(entry[index_name]))\n",
    "  \n",
    "    return batch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Function that is provided by samples and stacks them together to form tensors.\n",
    "    We call padding function here. \n",
    "    \n",
    "    :param batch: a list of sambles\n",
    "    :reutrn : input segments as tensors\n",
    "    \"\"\"\n",
    "    batch = pad_sequenses(batch, SPECIAL_TOKENS_IDS[-1])\n",
    "    \n",
    "    inputs = [torch.stack(list(map(lambda x: torch.from_numpy( \\\n",
    "        np.array(x[index_name])), batch)), dim=0) for index_name in batch[0].keys()]\n",
    "\n",
    "    inputs = [input_tensor.type(torch.LongTensor) for input_tensor in inputs]\n",
    "\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "        inputs = [input_tensor.cuda() for input_tensor in inputs]   \n",
    "\n",
    "    return inputs  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part is the creation of the DialogDataset objects and the call to the DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = DialogDataset(pairs_train) \n",
    "validation_set = DialogDataset(pairs_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 32 \n",
    "EVAL_BATCH_SIZE = 64 \n",
    "\n",
    "dataloader_train = DataLoader(training_set, batch_size=TRAIN_BATCH_SIZE, shuffle=False, \n",
    "                              collate_fn=custom_collate_fn, num_workers=0) \n",
    "\n",
    "dataloader_valid = DataLoader(validation_set, batch_size=EVAL_BATCH_SIZE, shuffle=False,\n",
    "                              collate_fn=custom_collate_fn, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of the batch sizes was made carefully as the model along with the batch of the biggest sequence length must be able to fit in RAM memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure\n",
    "\n",
    "As we proceed with the training we denifed a few helpful methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer):\n",
    "    \"\"\"\n",
    "    Handles forward and backward pass for an epoch.\n",
    "    \n",
    "    :return : the cumulative loss of the epoch\n",
    "    \"\"\"\n",
    "    epoch_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for i_batch, (input_ids, attention_mask, category_ids, label_ids) in enumerate(dataloader):\n",
    "        \n",
    "        loss, logits = model(input_ids, attention_mask, category_ids, labels=label_ids)\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()  \n",
    "        \n",
    "    return epoch_loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    \"\"\"\n",
    "    Handles forward pass on the validation set\n",
    "    \n",
    "    :return : cumulative loss on validation set\n",
    "    \"\"\"\n",
    "    epoch_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i_batch, (input_ids, attention_mask, category_ids, label_ids) in enumerate(dataloader):\n",
    "            \n",
    "            loss, logits = model(input_ids, attention_mask, category_ids, labels=label_ids)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **_train_** and **_evaluate_** functions perform the passes needed for an epoch, while **_early_stopping_** checks the generalization error produced in the validation set and based on some patience parameters it makes decision whether to stop training procudure or not. If it find a new min in validation loss it calls **_checkpoint_state_** which saves the model's weights along other configs into a specified directory.\n",
    "\n",
    "To mention as a fact only the weights of the model when checkpointed occupied almost 500MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint_state(model, tokenizer, output_dir=None):\n",
    "    \"\"\"\n",
    "    Function that saves models weights and configuration as well as tokenizer's voc.\n",
    "    \n",
    "    :param model: to checkpoint\n",
    "    :param tokenizer: to checkpoint\n",
    "    :param output_dir: directory where checkpointed files will be created\n",
    "    :retun :\n",
    "    \"\"\"\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = 'metalwoz-v1/checkpoint_{}'.format(datetime.now().strftime(TIME_FORMAT))\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(output_dir)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "    output_config_file = os.path.join(output_dir, CONFIG_NAME)  \n",
    "    \n",
    "    torch.save(model.state_dict(), output_model_file) # checkpoint weights\n",
    "    model.config.to_json_file(output_config_file)     # configuration\n",
    "    tokenizer.save_vocabulary(output_dir)             # vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingException(Exception):\n",
    "    \"\"\"\n",
    "    Wrapper class to denote a specified exception for Early Stopping\n",
    "    \"\"\"\n",
    "    def __init__(self, message):\n",
    "\n",
    "        super().__init__(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping(min_loss, cur_patience, max_patience, epoch_eval_loss):\n",
    "    \"\"\"\n",
    "    Function that checks validation loss and if it has a new min value saves a checkpoint\n",
    "    otherwise updates the patience and if it is exhausted raise an exception\n",
    "    \"\"\"\n",
    "    if (epoch_eval_loss >= min_loss):  \n",
    "        cur_patience += 1\n",
    "        if (cur_patience >= max_patience):\n",
    "            raise EarlyStoppingException(\"Execution terminated due to Early Stopping\")\n",
    "    else:\n",
    "        print(\"New min validation loss}\")\n",
    "        checkpoint_state(model, tokenizer) # checkpointing\n",
    "        print(\"New checkpoint created\")\n",
    "        min_loss, cur_patience = epoch_eval_loss, 0    \n",
    "        \n",
    "    return min_loss, cur_patience    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally **_training_procedure_** uses all those functions to perform the training.\n",
    "\n",
    "The optimizer that we used was Adam (with a learning rate of 0.0001) which has proven to be quite faster to converge than vanillia SGD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_procedure(model, epochs, dataloader_train, dataloader_valid, lr, use_cuda, min_loss, max_patience, cur_patience):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr) # , weight_decay=0.001 # TODO review those values\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_loss = train(model, dataloader_train, optimizer) / len(dataloader_train)\n",
    "        print(\"Loss on train set: \\t\\t epoch {} : {:.4f}\".format(epoch, train_loss))\n",
    "\n",
    "        eval_loss = evaluate(model, dataloader_valid) / len(dataloader_valid)     \n",
    "        print(\"Loss on validation set: \\t epoch {} : {:.4f}\".format(epoch, eval_loss))    \n",
    "\n",
    "        try:\n",
    "            min_loss, cur_patience = early_stopping(min_loss, cur_patience, max_patience, eval_loss) \n",
    "        except EarlyStoppingException as e:\n",
    "            print(\"{} at epoch: {}\".format(e, epoch))\n",
    "            break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a fine-tuning process as we start with a model with pretrained weights. Usually fine-tunig is performed for a small number of epochs. Here we set a maximum of 10 epochs and we monitor validation loss in order to early stop the procedure with a patience of 3 epochs.\n",
    "\n",
    "The act of training took place in Kaggle which provides access to GPUs accelarators as it was impossible to be handled by a commodity GPU. Even with a strong GPU like Tesla P100 every training epoch needed a bit less than an hour to be finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on train set: \t\t epoch 0 : 6.6461\n",
      "Loss on validation set: \t epoch 0 : 6.3160\n",
      "New min validation loss}\n",
      "New checkpoint created\n",
      "Loss on train set: \t\t epoch 1 : 6.1977\n",
      "Loss on validation set: \t epoch 1 : 5.8961\n",
      "New min validation loss}\n",
      "New checkpoint created\n",
      "Loss on train set: \t\t epoch 2 : 5.9382\n",
      "Loss on validation set: \t epoch 2 : 5.8530\n",
      "New min validation loss}\n",
      "New checkpoint created\n",
      "Loss on train set: \t\t epoch 3 : 5.8087\n",
      "Loss on validation set: \t epoch 3 : 5.8559\n",
      "Loss on train set: \t\t epoch 4 : 5.7147\n",
      "Loss on validation set: \t epoch 4 : 5.8270\n",
      "New min validation loss}\n",
      "New checkpoint created\n",
      "Loss on train set: \t\t epoch 5 : 5.6415\n",
      "Loss on validation set: \t epoch 5 : 5.8311\n",
      "Loss on train set: \t\t epoch 6 : 5.5931\n",
      "Loss on validation set: \t epoch 6 : 5.8536\n",
      "Loss on train set: \t\t epoch 7 : 5.5158\n",
      "Loss on validation set: \t epoch 7 : 5.8668\n",
      "Execution terminated due to Early Stopping at epoch: 7\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 1e-4\n",
    "min_loss, max_patience, cur_patience = np.inf, 3, 0\n",
    "\n",
    "train_procedure(model, epochs, dataloader_train, dataloader_valid, lr, use_cuda, min_loss, max_patience, cur_patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above execution results(which is on a sample of the data) and the experiments that we ran on Kaggle we saw that in general the model could not reduce the high values of CrossEntropyLoss. \n",
    "\n",
    "We expected the model to reduce the loss on validation set in the first few epochs and then continue reducing training loss (overfitting area). However after some epochs and with different settings tested the loss get stucked around a high value. \n",
    "\n",
    "Thus we assume that the inability of the model to learn properly is probably caused by a bug in the developed code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "We continue our work with the mechanisms needed in order to interact with the bot. At the core of those mechanisms is the decoding procedure.  \n",
    "\n",
    "The simpler approach when having the output of the model is to pass them through the softmax activation function. turning them so to probabilities and then pick the word with the highest value. \n",
    "\n",
    "However choosing the most probable word or sequences of words as in Beam-search approach doesn't seems to functions well. In fact the language that is produced by those methods is far from that that human produces as it is mentioned in [3]. \n",
    "\n",
    "To deal with this issue two new approaches came up in the past year. \n",
    "\n",
    "Top-k samples each responded word out of the k most probable tokens based on their probabilities, while the top-p approach samples from the most probable tokens that have a cumulative probability of p. This gives it the ability to focus on the crusial portion of the probability function either this is broad or narrow. \n",
    "\n",
    "Those methods along with greedy approach are implemented in the **_decoding_** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(history, tokenizer, reply_so_far):\n",
    "    \"\"\"\n",
    "    Functions that creates the various sequements required by the model as input\n",
    "    from the history and the reply developed so far.\n",
    "    \n",
    "    :return : segments as tensors, no padding/masking is needed as we have single input \n",
    "    \"\"\"\n",
    "    history = [tokenizer.encode(phrase) for phrase in history]\n",
    "    \n",
    "    instance = create_model_inputs(history, reply_so_far, tokenizer, with_eos=False)\n",
    "    \n",
    "    input_ids = torch.tensor(instance[\"input_ids\"]).unsqueeze(0)\n",
    "    token_type_ids = torch.tensor(instance[\"token_type_ids\"]).unsqueeze(0)\n",
    "    \n",
    "    return input_ids, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding(probs, logits, method=\"top_p\"):\n",
    "    \"\"\"\n",
    "    Functions that selects the next token to be emmited. Three different approaches are implemented: \n",
    "    \n",
    "    Greedy: the most probable token is selected.\n",
    "    Top-k : sample out of the k most probable tokens based on their probabilities \n",
    "    Top-p : sample from the more probable tokens that have a cumulative probability of p. \n",
    "    \n",
    "    :param logits: \n",
    "    :param method: the decoding method to be used, Values={'greedy', 'top_k', 'top_p'}\n",
    "    :return: the selected token\n",
    "    \"\"\"\n",
    "    top_k = 40 # sample from the 100 most probable tokens based on their probs\n",
    "    top_p = 0.9 # sample from the n most probable tokens that have a cumulative probability at least 0.9 \n",
    "    \n",
    "    if method == \"greedy\":\n",
    "        return torch.argmax(probs).item()\n",
    "    \n",
    "    elif method == \"top_k\":        \n",
    "        prob_k = probs.topk(top_k)[0][-1].item() # value of the 100th most probable\n",
    "        probs[probs < prob_k] = 0   # cut off the tail  \n",
    "        \n",
    "    elif method == \"top_p\":\n",
    "        probs_sorted, probs_indexes = probs.sort(dim=-1, descending=True) # start the cumulation from the most probable token in descending order\n",
    "        cum_probs = probs_sorted.cumsum(dim=-1)\n",
    "        \n",
    "        indices = cum_probs > top_p \n",
    "        indices[1:] = indices[:-1].clone()\n",
    "        indices[0] = 0 # at least one token is preserved \n",
    "        \n",
    "        probs[probs_indexes[indices]] = 0\n",
    "    \n",
    "    word = torch.multinomial(probs, 1).item()\n",
    "    # TODO handle the case that special token was emitted in the first pick\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **_infer_answer_** generates word by word the bot answer, based on user input and previous history. At each iteration history, which includes the so far model emmited words, is passed again into the model and the next token is generated until we find a special token or we reach the predifined max sentence length.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_answer(history, model, tokenizer, method=\"top_p\"):\n",
    "    \"\"\"\n",
    "    Function that generates word by word the bot answer, based on user input and previous history.\n",
    "    \n",
    "    :param history: a list of past sentences and last user's input, in plain text\n",
    "    :param model: the model to be used for inference\n",
    "    :return: a list with the words of the answer in plain text \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    reply_so_far = []\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i in range(max_sentence_length):\n",
    "            \n",
    "            input_ids, category_ids = format_input(history, tokenizer, reply_so_far)\n",
    "            outputs = model(input_ids=input_ids, token_type_ids=category_ids)\n",
    "            logits = outputs[0]\n",
    "            logits = logits[0, -1, :] / temperature # keep last \n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            word = decoding(probs, logits, method=method) \n",
    "            \n",
    "            if word in SPECIAL_TOKENS_IDS: # we stop inference if we find a special token without emitting this token\n",
    "                break\n",
    "            reply_so_far.append(word)\n",
    "            \n",
    "        answer_text = tokenizer.decode(reply_so_far, skip_special_tokens=True)    \n",
    "        return answer_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last of our functions we gather user input and we append it in history then we pass it as input in the model and we expect the answer. The answer is appended in the history as well and after that a new round of interaction may start.\n",
    "\n",
    "We also have the ability to choose between the aforementioned decoding methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_with_bot(model, tokenizer, method='top_p'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    bot_prompt = \"bot:>>> \"\n",
    "    user_prompt = \"user:>>> \"\n",
    "\n",
    "    history = []\n",
    "    print(bot_prompt + \"Hello how may I help you?\")\n",
    "    user_input = input(user_prompt)\n",
    "    \n",
    "    while user_input != \"\\q\": # TODO check if we need to truncate user input to not exceed max_length\n",
    "        \n",
    "        history.append(user_input)\n",
    "        answer = infer_answer(history, model, tokenizer, method=method)\n",
    "        history.append(answer)\n",
    "        \n",
    "        history = history[-(2*max_history+1):]  # keep the same history as in the training \n",
    "        \n",
    "        print(bot_prompt + answer)\n",
    "        \n",
    "        user_input = input(user_prompt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we initiate a model with some previously checkpointed weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New tokens added to model: 5\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"metalwoz-v1/checkpoint_20191115_1645\"\n",
    "\n",
    "model_loaded ,tokenizer_loaded  = load_checkpoint(checkpoint_dir)\n",
    "\n",
    "add_special_tokens_(model_loaded, tokenizer_loaded, SPECIAL_TOKENS_DICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction with the bot\n",
    "\n",
    "As we expected based on what we observed earlier the infered sentences are not good. \n",
    "\n",
    "We tried with different decoding approaches. Sampling methods present similar results while the greedy approach produced mainly repetive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bot:>>> Hello how may I help you?\n",
      "user:>>> can you please tell me what time it is?\n",
      "bot:>>> will it be on pm, may you be? do you? pm\n",
      "user:>>> this is not really helpful. could you tell me again?\n",
      "bot:>>> $ do? august, pm you will in bpjs? what will your help you, what\n",
      "user:>>> nevermind bye!\n",
      "bot:>>> the next one. account, / 4 you have a friend? time\n",
      "user:>>> \\q\n"
     ]
    }
   ],
   "source": [
    "interact_with_bot(model_loaded, tokenizer_loaded, method='top_p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bot:>>> Hello how may I help you?\n",
      "user:>>> do you have any suggestions for a restaurant?\n",
      "bot:>>> i can go to any restaurant but your name?? it is your number. your date account?\n",
      "user:>>> just tell me a restaurant\n",
      "bot:>>> ??.. the\n",
      "user:>>> this isn't helpful could you repeat?\n",
      "bot:>>> the day the day you you, or in the microwave and the m -'what are you what\n",
      "user:>>> \\q\n"
     ]
    }
   ],
   "source": [
    "interact_with_bot(model_loaded, tokenizer_loaded, method='top_k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bot:>>> Hello how may I help you?\n",
      "user:>>> do you have any suggestions for a restaurant?\n",
      "bot:>>> ' a is a restaurant\n",
      "user:>>> is that a name?\n",
      "bot:>>> i am your can be a what do you will, ok ok? how am?'it '\n",
      "user:>>> ok whatever you say\n",
      "bot:>>> that was at your lap what are the weather, i\n",
      "user:>>> \\q\n"
     ]
    }
   ],
   "source": [
    "interact_with_bot(model_loaded, tokenizer_loaded, method='top_p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bot:>>> Hello how may I help you?\n",
      "user:>>> can you propose me a restaurant?\n",
      "bot:>>> lunch? dinner? dinner? dinner? dinner? dinner? dinner? dinner? dinner? dinner?\n",
      "user:>>> \\q\n"
     ]
    }
   ],
   "source": [
    "interact_with_bot(model_loaded, tokenizer_loaded, method='greedy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Transformers Library: https://github.com/huggingface/transformers\n",
    "\n",
    "[2] Medium Tutorial: https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313\n",
    "\n",
    "[3] The Curious Case of Neural Text Degeneration: https://arxiv.org/abs/1904.09751 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
